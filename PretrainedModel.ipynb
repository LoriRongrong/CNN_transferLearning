{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PretrainedModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jn3VxczgvpzY",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_NbRe2LvxqL",
        "outputId": "2e5cb70e-f0b3-463c-bb26-2cf43a79252f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)  "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2bXqn3ZTv2b9",
        "outputId": "fb77b028-b0c4-4981-80ac-a23dc4e430dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "drive_dir = '/content/gdrive/My Drive/COGS181 Datasets/'"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hjw9WAiXvywS",
        "colab": {}
      },
      "source": [
        "# model_name = \"resnet\"# \"alexnet\", \"densenet\", 'vgg', 'resnet'\n",
        "# num_epochs = 25 # reduce num_epochs to see if solve overfitting\n",
        "# batch_size = 32\n",
        "# num_workers = 2\n",
        "# feature_extract = True\n",
        "# datafile = 'fma'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BAjl-xU5v1sl",
        "colab": {}
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx][0]\n",
        "        y = self.data[idx][1]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        x = torch.Tensor(x)\n",
        "\n",
        "        return (x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qqz5tQd2wK3F",
        "colab": {}
      },
      "source": [
        "def data_preparation(datafile):\n",
        "    if datafile == 'fma':\n",
        "        # FMA\n",
        "        FMA_data = pickle.load(open(drive_dir + 'FMA/FMA_165x32_normalized.train', 'rb'))\n",
        "        FMA_trainset = Dataset(FMA_data)\n",
        "        FMA_trainsize = len(FMA_trainset)\n",
        "        FMA_trainloader = torch.utils.data.DataLoader(FMA_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        FMA_data = pickle.load(open(drive_dir + 'FMA/FMA_165x32_normalized.test', 'rb'))\n",
        "        FMA_testset = Dataset(FMA_data)\n",
        "        FMA_testsize = len(FMA_testset)\n",
        "        FMA_testloader = torch.utils.data.DataLoader(FMA_testset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        FMA_classes = ['Electronic', 'Hip-Hop', 'Rock', 'Folk', 'Punk', 'Old-Time / Historic', 'Experimental', 'Soundtrack', 'Classical', 'Pop', 'Jazz', 'Avant-Garde', 'Psych-Rock', 'Indie-Rock', 'Ambient Electronic', 'Alternative', 'International', 'Instrumental', 'Trip-Hop', 'Metal']\n",
        "        return FMA_trainloader, FMA_testloader, FMA_classes, FMA_trainsize, FMA_testsize\n",
        "    elif datafile == 'gtzan':\n",
        "        # GTZAN\n",
        "        GTZAN_data = pickle.load(open(drive_dir + 'GTZAN/GTZAN_165x32_normalized.train', 'rb'))\n",
        "        GTZAN_trainset = Dataset(GTZAN_data)\n",
        "        GTZAN_trainsize = len(GTZAN_trainset)\n",
        "        GTZAN_trainloader = torch.utils.data.DataLoader(GTZAN_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        GTZAN_data = pickle.load(open(drive_dir + 'GTZAN/GTZAN_165x32_normalized.test', 'rb'))\n",
        "        GTZAN_testset = Dataset(GTZAN_data)\n",
        "        GTZAN_testsize = len(GTZAN_testset)\n",
        "        GTZAN_testloader = torch.utils.data.DataLoader(GTZAN_testset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        GTZAN_classes = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "        return GTZAN_trainloader, GTZAN_testloader, GTZAN_classes, GTZAN_trainsize, GTZAN_testsize\n",
        "    elif datafile =='pmc':\n",
        "      # PMC\n",
        "        PMC_data = pickle.load(open(drive_dir + 'PMC/PMC_165x32_normalized.train', 'rb'))\n",
        "        PMC_trainset = Dataset(PMC_data)\n",
        "        PMC_trainsize = len(PMC_trainset)\n",
        "        PMC_trainloader = torch.utils.data.DataLoader(PMC_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "        PMC_data = pickle.load(open(drive_dir + 'PMC/PMC_165x32_normalized.test', 'rb'))\n",
        "        PMC_testset = Dataset(PMC_data)\n",
        "        PMC_testsize = len(PMC_testset)\n",
        "        PMC_testloader = torch.utils.data.DataLoader(PMC_testset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "        PMC_classes = ['sad', 'happy', 'scary', 'peaceful']\n",
        "        return PMC_trainloader, PMC_testloader, PMC_classes, PMC_trainsize, PMC_testsize\n",
        "\n",
        "    else: \n",
        "        # RAVDESS\n",
        "        RAVDESS_data = pickle.load(open(drive_dir + 'RAVDESS/RAVDESS_165x32_normalized.train', 'rb'))\n",
        "        RAVDESS_trainset = Dataset(RAVDESS_data)\n",
        "        RAVDESS_trainsize = len(RAVDESS_trainset)\n",
        "        RAVDESS_trainloader = torch.utils.data.DataLoader(RAVDESS_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        RAVDESS_data = pickle.load(open(drive_dir + 'RAVDESS/RAVDESS_165x32_normalized.test', 'rb'))\n",
        "        RAVDESS_testset = Dataset(RAVDESS_data)\n",
        "        RAVDESS_testsize = len(RAVDESS_testset)\n",
        "        RAVDESS_testloader = torch.utils.data.DataLoader(RAVDESS_testset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "        RAVDESS_classes = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
        "        return RAVDESS_trainloader, RAVDESS_testloader, RAVDESS_classes, RAVDESS_trainsize, RAVDESS_testsize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ono_rFoDAOaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = [\"resnet\", 'vgg']# \"alexnet\", \"densenet\", 'vgg', 'resnet'\n",
        "ep = [30] # reduce num_epochs to see if solve overfitting\n",
        "bs = [32]\n",
        "num_workers = 2\n",
        "feature = [True, False]\n",
        "f = ['pmc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyAUow1UAXsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for model in model_name:\n",
        "#   for f in datafile:\n",
        "#     for ep in num_epochs:\n",
        "#       for bs in batch_size:\n",
        "#         for k in range(3):  # cross/val\n",
        "#           trainloader, testloader, classes, trainsize, testsize = data_preparation(datafile)\n",
        "#           num_classes = len(classes)\n",
        "#           model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "#           model_ft.to(device)\n",
        "#           # Observe that all parameters are being optimized\n",
        "\n",
        "#           optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "#           # Setup the loss fxn\n",
        "#           criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#           # Train and evaluate\n",
        "#           model_ft, avg_loss, avg_acc = train_model(model_ft, trainloader, \n",
        "#                                       criterion, optimizer_ft, epochs=num_epochs)\n",
        "#           print('training accuracy: ', model, f,' ', ep,' ' bs,' ', k)\n",
        "#           # Get test accuracy.\n",
        "#           correct_net = 0\n",
        "#           correct_cnn = 0\n",
        "#           total = 0\n",
        "#           with torch.no_grad():\n",
        "              \n",
        "#               for data in testloader:\n",
        "#                   images, labels = data\n",
        "#                   images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#                   outputs_cnn = model_ft(images)\n",
        "\n",
        "#                   _, predicted_cnn = torch.max(outputs_cnn.data, 1)\n",
        "#                   total += labels.size(0)\n",
        "\n",
        "#                   correct_cnn += (predicted_cnn == labels).sum().item()\n",
        "\n",
        "\n",
        "          # print('pre_trained: test acc: %d %%' % (\n",
        "          #     100 * correct_cnn / total))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CtHnxU5swMQG",
        "colab": {}
      },
      "source": [
        "\n",
        "# trainloader, testloader, classes, trainsize, testsize = data_preparation(datafile)\n",
        "# num_classes = len(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sTWXUzXzwNkA",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders,criterion, opt,epochs):\n",
        "    since = time.time()\n",
        "    avg_loss = []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    avg_acc = []\n",
        "    print_freq = 100\n",
        "#     model = model.cuda()\n",
        "    for epoch in range(epochs):\n",
        "        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        # print('-' * 10)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "\n",
        "        \n",
        "        for i, data in enumerate(dataloaders):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            opt.zero_grad()\n",
        "             # forward\n",
        "            # track history if only in train\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs,1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        # statistics\n",
        "            running_loss += loss.item()*inputs.size(0)\n",
        "            running_acc += torch.sum(preds == labels.data)\n",
        "#       \n",
        "        \n",
        "        losses = running_loss / trainsize\n",
        "        acc = running_acc / trainsize\n",
        "        # print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}, acc: {:.3f}'.format(\n",
        "            # epoch, i, losses, acc))\n",
        "        avg_loss.append(losses)\n",
        "        avg_acc.append(acc)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0      \n",
        "            \n",
        "    time_elapsed = time.time() - since\n",
        "    # print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    \"\"\"this is only useful if we want to combine training and eval states\"\"\"\n",
        "    # load best model weights\n",
        "#     model.load_state_dict(best_model_wts)\n",
        "    return model, avg_loss, avg_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6TBCqkWJwR3E",
        "colab": {}
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
        "        \n",
        "        \"\"\"this is uncomment if we want to freeze the inner layer\"\"\"\n",
        "        # freeze inner layer\n",
        "        if feature_extract:\n",
        "          for param in model_ft.parameters():\n",
        "              param.requires_grad = False  \n",
        "        \n",
        "        model_ft.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        \n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224 # what does it mean?\n",
        "    \n",
        "    \n",
        "    elif model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "#         set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        if feature_extract:\n",
        "          for param in model_ft.parameters():\n",
        "              param.requires_grad = False  \n",
        "        model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "        \n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "# model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "# print(model_ft)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F3169GtNwTJp",
        "colab": {}
      },
      "source": [
        "# model_ft = model_ft.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efFJmDMowUjv",
        "colab": {}
      },
      "source": [
        "# Gather the parameters to be optimized/updated in this run. \n",
        "# params_to_update = model_ft.parameters()\n",
        "# print(\"Params to learn:\")\n",
        "# if feature_extract:\n",
        "#     params_to_update = []\n",
        "#     for name,param in model_ft.named_parameters():\n",
        "#         if param.requires_grad == True:\n",
        "#             params_to_update.append(param)\n",
        "#             print(\"\\t\",name)\n",
        "# else:\n",
        "#     for name,param in model_ft.named_parameters():\n",
        "#         if param.requires_grad == True:\n",
        "#             print(\"\\t\",name)\n",
        "\n",
        "# # Observe that all parameters are being optimized\n",
        "\n",
        "# optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "# # Setup the loss fxn\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Train and evaluate\n",
        "# model_ft, avg_loss = train_model(model_ft, trainloader, \n",
        "#                              criterion, optimizer_ft, epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dw7DfpuTwWG8",
        "colab": {}
      },
      "source": [
        "# plt.plot(avg_loss, 'b', label='default')\n",
        "# print_freq = 100\n",
        "# plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
        "# plt.ylabel('avg. mini-batch loss')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSMBEHzcweMk",
        "colab": {}
      },
      "source": [
        "# # Get test accuracy.\n",
        "# correct_net = 0\n",
        "# correct_cnn = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "    \n",
        "#     for data in testloader:\n",
        "#         images, labels = data\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#         outputs_cnn = model_ft(images)\n",
        "\n",
        "#         _, predicted_cnn = torch.max(outputs_cnn.data, 1)\n",
        "#         total += labels.size(0)\n",
        "\n",
        "#         correct_cnn += (predicted_cnn == labels).sum().item()\n",
        "\n",
        "\n",
        "# print('pre_trained: Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "#     100 * correct_cnn / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L94a3E-Pwfgd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4b6ec358-af7f-4827-c107-43ae697d07cd"
      },
      "source": [
        "for model_name in model:\n",
        "  for datafile in f:\n",
        "    for feature_extract in feature:\n",
        "      for num_epochs in ep:\n",
        "        for batch_size in bs:\n",
        "        \n",
        "          trainloader, testloader, classes, trainsize, testsize = data_preparation(datafile)\n",
        "          num_classes = len(classes)\n",
        "          model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "          model_ft.to(device)\n",
        "          # Observe that all parameters are being optimized\n",
        "          params_to_update = model_ft.parameters()\n",
        "          optimizer_ft = optim.Adam(params_to_update, lr=0.001)\n",
        "          # Setup the loss fxn\n",
        "          criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "          # Train and evaluate\n",
        "          model_ft, avg_loss, avg_acc = train_model(model_ft, trainloader, \n",
        "                                      criterion, optimizer_ft, epochs=num_epochs)\n",
        "          print('feature extraction: ', feature_extract ,' training accuracy: ', 'model name: ', model_name,' data: ', datafile,' epoch: ', num_epochs,' batch_size: ', batch_size,' fold: ', k, ' train_acc: ', avg_acc[-1])\n",
        "          # Get test accuracy.\n",
        "          correct_net = 0\n",
        "          correct_cnn = 0\n",
        "          total = 0\n",
        "          with torch.no_grad():\n",
        "              \n",
        "              for data in testloader:\n",
        "                  images, labels = data\n",
        "                  images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                  outputs_cnn = model_ft(images)\n",
        "\n",
        "                  _, predicted_cnn = torch.max(outputs_cnn.data, 1)\n",
        "                  total += labels.size(0)\n",
        "\n",
        "                  correct_cnn += (predicted_cnn == labels).sum().item()\n",
        "\n",
        "          print('pre_trained: test acc: %d %%' % (100 * correct_cnn / total))\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature extraction:  True  training accuracy:  model name:  resnet  data:  pmc  epoch:  30  batch_size:  32  fold:  1  train_acc:  tensor(0.7381, device='cuda:0')\n",
            "pre_trained: test acc: 78 %\n",
            "feature extraction:  False  training accuracy:  model name:  resnet  data:  pmc  epoch:  30  batch_size:  32  fold:  1  train_acc:  tensor(0.7619, device='cuda:0')\n",
            "pre_trained: test acc: 93 %\n",
            "feature extraction:  True  training accuracy:  model name:  vgg  data:  pmc  epoch:  30  batch_size:  32  fold:  1  train_acc:  tensor(0.7302, device='cuda:0')\n",
            "pre_trained: test acc: 62 %\n",
            "feature extraction:  False  training accuracy:  model name:  vgg  data:  pmc  epoch:  30  batch_size:  32  fold:  1  train_acc:  tensor(0.3730, device='cuda:0')\n",
            "pre_trained: test acc: 40 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}