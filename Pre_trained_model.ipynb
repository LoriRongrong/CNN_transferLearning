{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= ''\n",
    "model_name = \"vgg\"# \"alexnet\", \"densenet\", 'vgg', 'resnet'\n",
    "num_classes = 10\n",
    "batch_size = 1\n",
    "num_epochs = 30\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][0]\n",
    "        y = self.data[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        x = torch.Tensor(x)\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "GTZAN_data = pickle.load(open('GTZAN_resized.train', 'rb'))\n",
    "trainset = GTZANDataset(GTZAN_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "GTZAN_data = pickle.load(open('GTZAN_resized.test', 'rb'))\n",
    "testset = GTZANDataset(GTZAN_data)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders_dict = {'train': trainloader, 'test':testloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [resnet, alexnet, vgg, squeezenet, densenet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders,criterion, opt,epochs):\n",
    "    since = time.time()\n",
    "    avg_loss = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    avg_acc = []\n",
    "    print_freq = 100\n",
    "    model = model.cuda()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "             # forward\n",
    "            # track history if only in train\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        # statistics\n",
    "            running_loss += loss.item() \n",
    "            running_acc += torch.sum(preds == labels.data)\n",
    "#             scheduler.step()\n",
    "            if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "                losses = running_loss / print_freq\n",
    "                acc = running_acc / print_freq\n",
    "                print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}, acc: {:.3f}'.format(\n",
    "                    epoch, i, losses, acc))\n",
    "                avg_loss.append(losses)\n",
    "                avg_acc.append(acc)\n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "            # deep copy the model\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU(inplace=True)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        \n",
    "        \"\"\"this is uncomment if we want to freeze the inner layer\"\"\"\n",
    "        # freeze inner layer\n",
    "#         for param in model_ft.parameters():\n",
    "#             param.requires_grad = False  # will this cause a problem for the first layer?\n",
    "        \n",
    "        model_ft.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224 # what does it mean?\n",
    "    \n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "#         set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "        model_ft.features[12] = nn.MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224 \n",
    "    \n",
    "    elif model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "#         set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.1.weight\n",
      "\t features.1.bias\n",
      "\t features.3.weight\n",
      "\t features.3.bias\n",
      "\t features.4.weight\n",
      "\t features.4.bias\n",
      "\t features.7.weight\n",
      "\t features.7.bias\n",
      "\t features.8.weight\n",
      "\t features.8.bias\n",
      "\t features.10.weight\n",
      "\t features.10.bias\n",
      "\t features.11.weight\n",
      "\t features.11.bias\n",
      "\t features.14.weight\n",
      "\t features.14.bias\n",
      "\t features.15.weight\n",
      "\t features.15.bias\n",
      "\t features.17.weight\n",
      "\t features.17.bias\n",
      "\t features.18.weight\n",
      "\t features.18.bias\n",
      "\t features.20.weight\n",
      "\t features.20.bias\n",
      "\t features.21.weight\n",
      "\t features.21.bias\n",
      "\t features.24.weight\n",
      "\t features.24.bias\n",
      "\t features.25.weight\n",
      "\t features.25.bias\n",
      "\t features.27.weight\n",
      "\t features.27.bias\n",
      "\t features.28.weight\n",
      "\t features.28.bias\n",
      "\t features.30.weight\n",
      "\t features.30.bias\n",
      "\t features.31.weight\n",
      "\t features.31.bias\n",
      "\t features.34.weight\n",
      "\t features.34.bias\n",
      "\t features.35.weight\n",
      "\t features.35.bias\n",
      "\t features.37.weight\n",
      "\t features.37.bias\n",
      "\t features.38.weight\n",
      "\t features.38.bias\n",
      "\t features.40.weight\n",
      "\t features.40.bias\n",
      "\t features.41.weight\n",
      "\t features.41.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/29\n",
      "----------\n",
      "[epoch: 0, i:    99] avg mini-batch loss: 2.353, acc: 0.110\n",
      "[epoch: 0, i:   199] avg mini-batch loss: 2.413, acc: 0.070\n",
      "[epoch: 0, i:   299] avg mini-batch loss: 2.348, acc: 0.110\n",
      "[epoch: 0, i:   399] avg mini-batch loss: 2.348, acc: 0.100\n",
      "[epoch: 0, i:   499] avg mini-batch loss: 2.334, acc: 0.110\n",
      "[epoch: 0, i:   599] avg mini-batch loss: 2.334, acc: 0.120\n",
      "[epoch: 0, i:   699] avg mini-batch loss: 2.294, acc: 0.130\n",
      "[epoch: 0, i:   799] avg mini-batch loss: 2.268, acc: 0.190\n",
      "Epoch 1/29\n",
      "----------\n",
      "[epoch: 1, i:    99] avg mini-batch loss: 2.192, acc: 0.220\n",
      "[epoch: 1, i:   199] avg mini-batch loss: 2.057, acc: 0.310\n",
      "[epoch: 1, i:   299] avg mini-batch loss: 2.217, acc: 0.230\n",
      "[epoch: 1, i:   399] avg mini-batch loss: 2.151, acc: 0.240\n",
      "[epoch: 1, i:   499] avg mini-batch loss: 2.187, acc: 0.170\n",
      "[epoch: 1, i:   599] avg mini-batch loss: 1.887, acc: 0.320\n",
      "[epoch: 1, i:   699] avg mini-batch loss: 2.122, acc: 0.220\n",
      "[epoch: 1, i:   799] avg mini-batch loss: 2.294, acc: 0.120\n",
      "Epoch 2/29\n",
      "----------\n",
      "[epoch: 2, i:    99] avg mini-batch loss: 1.845, acc: 0.400\n",
      "[epoch: 2, i:   199] avg mini-batch loss: 1.998, acc: 0.270\n",
      "[epoch: 2, i:   299] avg mini-batch loss: 2.038, acc: 0.250\n",
      "[epoch: 2, i:   399] avg mini-batch loss: 2.024, acc: 0.260\n",
      "[epoch: 2, i:   499] avg mini-batch loss: 2.061, acc: 0.180\n",
      "[epoch: 2, i:   599] avg mini-batch loss: 2.103, acc: 0.210\n",
      "[epoch: 2, i:   699] avg mini-batch loss: 2.172, acc: 0.170\n",
      "[epoch: 2, i:   799] avg mini-batch loss: 1.869, acc: 0.300\n",
      "Epoch 3/29\n",
      "----------\n",
      "[epoch: 3, i:    99] avg mini-batch loss: 1.701, acc: 0.340\n",
      "[epoch: 3, i:   199] avg mini-batch loss: 1.954, acc: 0.270\n",
      "[epoch: 3, i:   299] avg mini-batch loss: 1.939, acc: 0.300\n",
      "[epoch: 3, i:   399] avg mini-batch loss: 2.148, acc: 0.170\n",
      "[epoch: 3, i:   499] avg mini-batch loss: 1.718, acc: 0.360\n",
      "[epoch: 3, i:   599] avg mini-batch loss: 1.884, acc: 0.340\n",
      "[epoch: 3, i:   699] avg mini-batch loss: 1.966, acc: 0.300\n",
      "[epoch: 3, i:   799] avg mini-batch loss: 2.055, acc: 0.290\n",
      "Epoch 4/29\n",
      "----------\n",
      "[epoch: 4, i:    99] avg mini-batch loss: 1.784, acc: 0.300\n",
      "[epoch: 4, i:   199] avg mini-batch loss: 1.676, acc: 0.360\n",
      "[epoch: 4, i:   299] avg mini-batch loss: 1.660, acc: 0.390\n",
      "[epoch: 4, i:   399] avg mini-batch loss: 1.651, acc: 0.390\n",
      "[epoch: 4, i:   499] avg mini-batch loss: 1.912, acc: 0.300\n",
      "[epoch: 4, i:   599] avg mini-batch loss: 1.577, acc: 0.440\n",
      "[epoch: 4, i:   699] avg mini-batch loss: 1.711, acc: 0.400\n",
      "[epoch: 4, i:   799] avg mini-batch loss: 1.670, acc: 0.410\n",
      "Epoch 5/29\n",
      "----------\n",
      "[epoch: 5, i:    99] avg mini-batch loss: 1.446, acc: 0.530\n",
      "[epoch: 5, i:   199] avg mini-batch loss: 1.528, acc: 0.400\n",
      "[epoch: 5, i:   299] avg mini-batch loss: 1.561, acc: 0.370\n",
      "[epoch: 5, i:   399] avg mini-batch loss: 1.575, acc: 0.440\n",
      "[epoch: 5, i:   499] avg mini-batch loss: 1.446, acc: 0.460\n",
      "[epoch: 5, i:   599] avg mini-batch loss: 1.794, acc: 0.390\n",
      "[epoch: 5, i:   699] avg mini-batch loss: 1.489, acc: 0.470\n",
      "[epoch: 5, i:   799] avg mini-batch loss: 1.739, acc: 0.330\n",
      "Epoch 6/29\n",
      "----------\n",
      "[epoch: 6, i:    99] avg mini-batch loss: 1.447, acc: 0.480\n",
      "[epoch: 6, i:   199] avg mini-batch loss: 1.573, acc: 0.460\n",
      "[epoch: 6, i:   299] avg mini-batch loss: 1.485, acc: 0.480\n",
      "[epoch: 6, i:   399] avg mini-batch loss: 1.301, acc: 0.490\n",
      "[epoch: 6, i:   499] avg mini-batch loss: 1.432, acc: 0.440\n",
      "[epoch: 6, i:   599] avg mini-batch loss: 1.276, acc: 0.550\n",
      "[epoch: 6, i:   699] avg mini-batch loss: 1.324, acc: 0.490\n",
      "[epoch: 6, i:   799] avg mini-batch loss: 2.104, acc: 0.340\n",
      "Epoch 7/29\n",
      "----------\n",
      "[epoch: 7, i:    99] avg mini-batch loss: 1.561, acc: 0.480\n",
      "[epoch: 7, i:   199] avg mini-batch loss: 1.610, acc: 0.440\n",
      "[epoch: 7, i:   299] avg mini-batch loss: 1.780, acc: 0.360\n",
      "[epoch: 7, i:   399] avg mini-batch loss: 1.449, acc: 0.460\n",
      "[epoch: 7, i:   499] avg mini-batch loss: 1.521, acc: 0.480\n",
      "[epoch: 7, i:   599] avg mini-batch loss: 1.427, acc: 0.530\n",
      "[epoch: 7, i:   699] avg mini-batch loss: 1.121, acc: 0.530\n",
      "[epoch: 7, i:   799] avg mini-batch loss: 1.529, acc: 0.490\n",
      "Epoch 8/29\n",
      "----------\n",
      "[epoch: 8, i:    99] avg mini-batch loss: 0.983, acc: 0.680\n",
      "[epoch: 8, i:   199] avg mini-batch loss: 0.937, acc: 0.660\n",
      "[epoch: 8, i:   299] avg mini-batch loss: 1.436, acc: 0.500\n",
      "[epoch: 8, i:   399] avg mini-batch loss: 1.382, acc: 0.480\n",
      "[epoch: 8, i:   499] avg mini-batch loss: 1.329, acc: 0.520\n",
      "[epoch: 8, i:   599] avg mini-batch loss: 1.300, acc: 0.550\n",
      "[epoch: 8, i:   699] avg mini-batch loss: 1.130, acc: 0.630\n",
      "[epoch: 8, i:   799] avg mini-batch loss: 1.386, acc: 0.530\n",
      "Epoch 9/29\n",
      "----------\n",
      "[epoch: 9, i:    99] avg mini-batch loss: 1.039, acc: 0.620\n",
      "[epoch: 9, i:   199] avg mini-batch loss: 0.813, acc: 0.740\n",
      "[epoch: 9, i:   299] avg mini-batch loss: 0.987, acc: 0.640\n",
      "[epoch: 9, i:   399] avg mini-batch loss: 0.896, acc: 0.670\n",
      "[epoch: 9, i:   499] avg mini-batch loss: 1.183, acc: 0.580\n",
      "[epoch: 9, i:   599] avg mini-batch loss: 0.920, acc: 0.670\n",
      "[epoch: 9, i:   699] avg mini-batch loss: 1.233, acc: 0.590\n",
      "[epoch: 9, i:   799] avg mini-batch loss: 1.139, acc: 0.560\n",
      "Epoch 10/29\n",
      "----------\n",
      "[epoch: 10, i:    99] avg mini-batch loss: 0.796, acc: 0.730\n",
      "[epoch: 10, i:   199] avg mini-batch loss: 0.736, acc: 0.740\n",
      "[epoch: 10, i:   299] avg mini-batch loss: 0.884, acc: 0.740\n",
      "[epoch: 10, i:   399] avg mini-batch loss: 0.934, acc: 0.660\n",
      "[epoch: 10, i:   499] avg mini-batch loss: 1.004, acc: 0.620\n",
      "[epoch: 10, i:   599] avg mini-batch loss: 0.954, acc: 0.680\n",
      "[epoch: 10, i:   699] avg mini-batch loss: 1.127, acc: 0.580\n",
      "[epoch: 10, i:   799] avg mini-batch loss: 0.851, acc: 0.690\n",
      "Epoch 11/29\n",
      "----------\n",
      "[epoch: 11, i:    99] avg mini-batch loss: 0.852, acc: 0.710\n",
      "[epoch: 11, i:   199] avg mini-batch loss: 0.716, acc: 0.760\n",
      "[epoch: 11, i:   299] avg mini-batch loss: 0.993, acc: 0.660\n",
      "[epoch: 11, i:   399] avg mini-batch loss: 0.894, acc: 0.680\n",
      "[epoch: 11, i:   499] avg mini-batch loss: 1.017, acc: 0.640\n",
      "[epoch: 11, i:   599] avg mini-batch loss: 1.204, acc: 0.600\n",
      "[epoch: 11, i:   699] avg mini-batch loss: 1.069, acc: 0.640\n",
      "[epoch: 11, i:   799] avg mini-batch loss: 1.523, acc: 0.590\n",
      "Epoch 12/29\n",
      "----------\n",
      "[epoch: 12, i:    99] avg mini-batch loss: 0.917, acc: 0.650\n",
      "[epoch: 12, i:   199] avg mini-batch loss: 0.800, acc: 0.700\n",
      "[epoch: 12, i:   299] avg mini-batch loss: 0.637, acc: 0.770\n",
      "[epoch: 12, i:   399] avg mini-batch loss: 1.089, acc: 0.680\n",
      "[epoch: 12, i:   499] avg mini-batch loss: 0.564, acc: 0.880\n",
      "[epoch: 12, i:   599] avg mini-batch loss: 1.070, acc: 0.680\n",
      "[epoch: 12, i:   699] avg mini-batch loss: 0.758, acc: 0.700\n",
      "[epoch: 12, i:   799] avg mini-batch loss: 0.816, acc: 0.690\n",
      "Epoch 13/29\n",
      "----------\n",
      "[epoch: 13, i:    99] avg mini-batch loss: 0.498, acc: 0.890\n",
      "[epoch: 13, i:   199] avg mini-batch loss: 0.522, acc: 0.800\n",
      "[epoch: 13, i:   299] avg mini-batch loss: 0.404, acc: 0.860\n",
      "[epoch: 13, i:   399] avg mini-batch loss: 0.419, acc: 0.870\n",
      "[epoch: 13, i:   499] avg mini-batch loss: 0.907, acc: 0.710\n",
      "[epoch: 13, i:   599] avg mini-batch loss: 0.963, acc: 0.710\n",
      "[epoch: 13, i:   699] avg mini-batch loss: 0.709, acc: 0.720\n",
      "[epoch: 13, i:   799] avg mini-batch loss: 0.469, acc: 0.820\n",
      "Epoch 14/29\n",
      "----------\n",
      "[epoch: 14, i:    99] avg mini-batch loss: 0.486, acc: 0.880\n",
      "[epoch: 14, i:   199] avg mini-batch loss: 1.258, acc: 0.610\n",
      "[epoch: 14, i:   299] avg mini-batch loss: 1.020, acc: 0.670\n",
      "[epoch: 14, i:   399] avg mini-batch loss: 0.586, acc: 0.760\n",
      "[epoch: 14, i:   499] avg mini-batch loss: 0.984, acc: 0.630\n",
      "[epoch: 14, i:   599] avg mini-batch loss: 0.681, acc: 0.760\n",
      "[epoch: 14, i:   699] avg mini-batch loss: 1.128, acc: 0.630\n",
      "[epoch: 14, i:   799] avg mini-batch loss: 0.840, acc: 0.720\n",
      "Epoch 15/29\n",
      "----------\n",
      "[epoch: 15, i:    99] avg mini-batch loss: 0.941, acc: 0.710\n",
      "[epoch: 15, i:   199] avg mini-batch loss: 0.568, acc: 0.800\n",
      "[epoch: 15, i:   299] avg mini-batch loss: 0.653, acc: 0.790\n",
      "[epoch: 15, i:   399] avg mini-batch loss: 0.973, acc: 0.680\n",
      "[epoch: 15, i:   499] avg mini-batch loss: 0.746, acc: 0.760\n",
      "[epoch: 15, i:   599] avg mini-batch loss: 0.765, acc: 0.750\n",
      "[epoch: 15, i:   699] avg mini-batch loss: 0.821, acc: 0.710\n",
      "[epoch: 15, i:   799] avg mini-batch loss: 0.339, acc: 0.930\n",
      "Epoch 16/29\n",
      "----------\n",
      "[epoch: 16, i:    99] avg mini-batch loss: 0.558, acc: 0.860\n",
      "[epoch: 16, i:   199] avg mini-batch loss: 0.388, acc: 0.860\n",
      "[epoch: 16, i:   299] avg mini-batch loss: 0.242, acc: 0.930\n",
      "[epoch: 16, i:   399] avg mini-batch loss: 0.544, acc: 0.800\n",
      "[epoch: 16, i:   499] avg mini-batch loss: 0.412, acc: 0.860\n",
      "[epoch: 16, i:   599] avg mini-batch loss: 0.321, acc: 0.910\n",
      "[epoch: 16, i:   699] avg mini-batch loss: 0.616, acc: 0.790\n",
      "[epoch: 16, i:   799] avg mini-batch loss: 0.816, acc: 0.740\n",
      "Epoch 17/29\n",
      "----------\n",
      "[epoch: 17, i:    99] avg mini-batch loss: 0.668, acc: 0.780\n",
      "[epoch: 17, i:   199] avg mini-batch loss: 0.306, acc: 0.870\n",
      "[epoch: 17, i:   299] avg mini-batch loss: 0.237, acc: 0.920\n",
      "[epoch: 17, i:   399] avg mini-batch loss: 0.285, acc: 0.910\n",
      "[epoch: 17, i:   499] avg mini-batch loss: 0.259, acc: 0.920\n",
      "[epoch: 17, i:   599] avg mini-batch loss: 0.532, acc: 0.830\n",
      "[epoch: 17, i:   699] avg mini-batch loss: 0.560, acc: 0.800\n",
      "[epoch: 17, i:   799] avg mini-batch loss: 0.645, acc: 0.790\n",
      "Epoch 18/29\n",
      "----------\n",
      "[epoch: 18, i:    99] avg mini-batch loss: 0.236, acc: 0.930\n",
      "[epoch: 18, i:   199] avg mini-batch loss: 0.320, acc: 0.860\n",
      "[epoch: 18, i:   299] avg mini-batch loss: 0.282, acc: 0.910\n",
      "[epoch: 18, i:   399] avg mini-batch loss: 0.136, acc: 0.950\n",
      "[epoch: 18, i:   499] avg mini-batch loss: 0.295, acc: 0.890\n",
      "[epoch: 18, i:   599] avg mini-batch loss: 0.734, acc: 0.810\n",
      "[epoch: 18, i:   699] avg mini-batch loss: 0.464, acc: 0.860\n",
      "[epoch: 18, i:   799] avg mini-batch loss: 0.248, acc: 0.910\n",
      "Epoch 19/29\n",
      "----------\n",
      "[epoch: 19, i:    99] avg mini-batch loss: 0.210, acc: 0.940\n",
      "[epoch: 19, i:   199] avg mini-batch loss: 0.276, acc: 0.930\n",
      "[epoch: 19, i:   299] avg mini-batch loss: 0.255, acc: 0.920\n",
      "[epoch: 19, i:   399] avg mini-batch loss: 0.157, acc: 0.970\n",
      "[epoch: 19, i:   499] avg mini-batch loss: 0.194, acc: 0.930\n",
      "[epoch: 19, i:   599] avg mini-batch loss: 0.198, acc: 0.940\n",
      "[epoch: 19, i:   699] avg mini-batch loss: 0.089, acc: 0.970\n",
      "[epoch: 19, i:   799] avg mini-batch loss: 0.091, acc: 0.970\n",
      "Epoch 20/29\n",
      "----------\n",
      "[epoch: 20, i:    99] avg mini-batch loss: 0.194, acc: 0.930\n",
      "[epoch: 20, i:   199] avg mini-batch loss: 0.215, acc: 0.950\n",
      "[epoch: 20, i:   299] avg mini-batch loss: 0.098, acc: 0.980\n",
      "[epoch: 20, i:   399] avg mini-batch loss: 0.063, acc: 0.970\n",
      "[epoch: 20, i:   499] avg mini-batch loss: 0.082, acc: 0.980\n",
      "[epoch: 20, i:   599] avg mini-batch loss: 0.191, acc: 0.920\n",
      "[epoch: 20, i:   699] avg mini-batch loss: 0.317, acc: 0.910\n",
      "[epoch: 20, i:   799] avg mini-batch loss: 0.311, acc: 0.900\n",
      "Epoch 21/29\n",
      "----------\n",
      "[epoch: 21, i:    99] avg mini-batch loss: 0.246, acc: 0.950\n",
      "[epoch: 21, i:   199] avg mini-batch loss: 0.330, acc: 0.930\n",
      "[epoch: 21, i:   299] avg mini-batch loss: 0.601, acc: 0.870\n",
      "[epoch: 21, i:   399] avg mini-batch loss: 0.366, acc: 0.900\n",
      "[epoch: 21, i:   499] avg mini-batch loss: 0.385, acc: 0.860\n",
      "[epoch: 21, i:   599] avg mini-batch loss: 0.733, acc: 0.810\n",
      "[epoch: 21, i:   699] avg mini-batch loss: 0.457, acc: 0.840\n",
      "[epoch: 21, i:   799] avg mini-batch loss: 0.313, acc: 0.890\n",
      "Epoch 22/29\n",
      "----------\n",
      "[epoch: 22, i:    99] avg mini-batch loss: 0.092, acc: 0.960\n",
      "[epoch: 22, i:   199] avg mini-batch loss: 0.141, acc: 0.970\n",
      "[epoch: 22, i:   299] avg mini-batch loss: 0.148, acc: 0.960\n",
      "[epoch: 22, i:   399] avg mini-batch loss: 0.219, acc: 0.910\n",
      "[epoch: 22, i:   499] avg mini-batch loss: 0.441, acc: 0.890\n",
      "[epoch: 22, i:   599] avg mini-batch loss: 0.351, acc: 0.900\n",
      "[epoch: 22, i:   699] avg mini-batch loss: 0.397, acc: 0.880\n",
      "[epoch: 22, i:   799] avg mini-batch loss: 0.532, acc: 0.810\n",
      "Epoch 23/29\n",
      "----------\n",
      "[epoch: 23, i:    99] avg mini-batch loss: 0.360, acc: 0.900\n",
      "[epoch: 23, i:   199] avg mini-batch loss: 0.233, acc: 0.920\n",
      "[epoch: 23, i:   299] avg mini-batch loss: 0.128, acc: 0.950\n",
      "[epoch: 23, i:   399] avg mini-batch loss: 0.384, acc: 0.880\n",
      "[epoch: 23, i:   499] avg mini-batch loss: 0.211, acc: 0.940\n",
      "[epoch: 23, i:   599] avg mini-batch loss: 0.303, acc: 0.890\n",
      "[epoch: 23, i:   699] avg mini-batch loss: 0.207, acc: 0.940\n",
      "[epoch: 23, i:   799] avg mini-batch loss: 0.119, acc: 0.950\n",
      "Epoch 24/29\n",
      "----------\n",
      "[epoch: 24, i:    99] avg mini-batch loss: 0.065, acc: 0.980\n",
      "[epoch: 24, i:   199] avg mini-batch loss: 0.027, acc: 0.990\n",
      "[epoch: 24, i:   299] avg mini-batch loss: 0.338, acc: 0.960\n",
      "[epoch: 24, i:   399] avg mini-batch loss: 0.179, acc: 0.960\n",
      "[epoch: 24, i:   499] avg mini-batch loss: 0.099, acc: 0.950\n",
      "[epoch: 24, i:   599] avg mini-batch loss: 0.271, acc: 0.940\n",
      "[epoch: 24, i:   699] avg mini-batch loss: 0.128, acc: 0.970\n",
      "[epoch: 24, i:   799] avg mini-batch loss: 0.146, acc: 0.960\n",
      "Epoch 25/29\n",
      "----------\n",
      "[epoch: 25, i:    99] avg mini-batch loss: 0.054, acc: 0.970\n",
      "[epoch: 25, i:   199] avg mini-batch loss: 0.682, acc: 0.770\n",
      "[epoch: 25, i:   299] avg mini-batch loss: 0.242, acc: 0.930\n",
      "[epoch: 25, i:   399] avg mini-batch loss: 0.098, acc: 0.960\n",
      "[epoch: 25, i:   499] avg mini-batch loss: 0.166, acc: 0.940\n",
      "[epoch: 25, i:   599] avg mini-batch loss: 0.220, acc: 0.950\n",
      "[epoch: 25, i:   699] avg mini-batch loss: 0.598, acc: 0.850\n",
      "[epoch: 25, i:   799] avg mini-batch loss: 0.285, acc: 0.890\n",
      "Epoch 26/29\n",
      "----------\n",
      "[epoch: 26, i:    99] avg mini-batch loss: 0.111, acc: 0.970\n",
      "[epoch: 26, i:   199] avg mini-batch loss: 0.048, acc: 0.990\n",
      "[epoch: 26, i:   299] avg mini-batch loss: 0.100, acc: 0.960\n",
      "[epoch: 26, i:   399] avg mini-batch loss: 0.068, acc: 0.990\n",
      "[epoch: 26, i:   499] avg mini-batch loss: 0.097, acc: 0.940\n",
      "[epoch: 26, i:   599] avg mini-batch loss: 0.356, acc: 0.910\n",
      "[epoch: 26, i:   699] avg mini-batch loss: 0.294, acc: 0.910\n",
      "[epoch: 26, i:   799] avg mini-batch loss: 0.131, acc: 0.960\n",
      "Epoch 27/29\n",
      "----------\n",
      "[epoch: 27, i:    99] avg mini-batch loss: 0.037, acc: 0.990\n",
      "[epoch: 27, i:   199] avg mini-batch loss: 0.018, acc: 0.990\n",
      "[epoch: 27, i:   299] avg mini-batch loss: 0.060, acc: 0.980\n",
      "[epoch: 27, i:   399] avg mini-batch loss: 0.007, acc: 1.000\n",
      "[epoch: 27, i:   499] avg mini-batch loss: 0.041, acc: 0.990\n",
      "[epoch: 27, i:   599] avg mini-batch loss: 0.005, acc: 1.000\n",
      "[epoch: 27, i:   699] avg mini-batch loss: 0.004, acc: 1.000\n",
      "[epoch: 27, i:   799] avg mini-batch loss: 0.005, acc: 1.000\n",
      "Epoch 28/29\n",
      "----------\n",
      "[epoch: 28, i:    99] avg mini-batch loss: 0.003, acc: 1.000\n",
      "[epoch: 28, i:   199] avg mini-batch loss: 0.001, acc: 1.000\n",
      "[epoch: 28, i:   299] avg mini-batch loss: 0.056, acc: 0.980\n",
      "[epoch: 28, i:   399] avg mini-batch loss: 0.033, acc: 0.980\n",
      "[epoch: 28, i:   499] avg mini-batch loss: 0.008, acc: 1.000\n",
      "[epoch: 28, i:   599] avg mini-batch loss: 0.003, acc: 1.000\n",
      "[epoch: 28, i:   699] avg mini-batch loss: 0.012, acc: 1.000\n",
      "[epoch: 28, i:   799] avg mini-batch loss: 0.042, acc: 0.980\n",
      "Epoch 29/29\n",
      "----------\n",
      "[epoch: 29, i:    99] avg mini-batch loss: 0.027, acc: 0.990\n",
      "[epoch: 29, i:   199] avg mini-batch loss: 0.013, acc: 0.990\n",
      "[epoch: 29, i:   299] avg mini-batch loss: 0.005, acc: 1.000\n",
      "[epoch: 29, i:   399] avg mini-batch loss: 0.037, acc: 0.990\n",
      "[epoch: 29, i:   499] avg mini-batch loss: 0.070, acc: 0.990\n",
      "[epoch: 29, i:   599] avg mini-batch loss: 0.299, acc: 0.920\n",
      "[epoch: 29, i:   699] avg mini-batch loss: 0.066, acc: 0.970\n",
      "[epoch: 29, i:   799] avg mini-batch loss: 0.212, acc: 0.910\n",
      "Training complete in 11m 22s\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001,momentum=0.9)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft,avg_loss = train_model(model_ft, trainloader, \n",
    "                             criterion, optimizer_ft, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'vgg_on_GTZAN_resized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_loss, 'b', label='default')\n",
    "print_freq = 100\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net: Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_ft(images)\n",
    "       \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Net: Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Accuracy of blues :  8 %\n",
      "Net Accuracy of classical :  0 %\n",
      "Net Accuracy of country :  5 %\n",
      "Net Accuracy of disco :  2 %\n",
      "Net Accuracy of hiphop :  8 %\n",
      "Net Accuracy of  jazz :  6 %\n",
      "Net Accuracy of metal : 14 %\n",
      "Net Accuracy of   pop : 56 %\n",
      "Net Accuracy of reggae :  1 %\n",
      "Net Accuracy of  rock :  0 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock')\n",
    "preds_net = []\n",
    "\n",
    "truths = []\n",
    "\n",
    "# Get test accuracy for each class.\n",
    "class_correct_net = list(0. for i in range(10))\n",
    "\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs_net = model_ft(images)\n",
    "        \n",
    "        _, predicted_net = torch.max(outputs_net, 1)\n",
    "        \n",
    "        c_net = (predicted_net == labels).squeeze()\n",
    "        \n",
    "        if type(c_net) is not list:\n",
    "            c_net = [c_net]\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            preds_net.append(predicted_net[i].item())\n",
    "            \n",
    "            truths.append(labels[i].item())\n",
    "#             confusion_matrix[actual][pred_net] += 1\n",
    "#             confusion_matrix_cnn[actual][pred_cnn] += 1\n",
    "            \n",
    "            label = labels[i]\n",
    "            class_correct_net[label] += c_net[i].item()\n",
    "            \n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Net Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct_net[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
