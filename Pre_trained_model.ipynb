{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= ''\n",
    "model_name = 'vgg'\n",
    "num_classes = 10\n",
    "batch_size = 1\n",
    "num_epochs = 40\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][0]\n",
    "        y = self.data[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        x = torch.Tensor(x)\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "GTZAN_data = pickle.load(open('GTZAN.train', 'rb'))\n",
    "trainset = GTZANDataset(GTZAN_data)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "GTZAN_data = pickle.load(open('GTZAN.test', 'rb'))\n",
    "testset = GTZANDataset(GTZAN_data)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders_dict = {'train': trainloader, 'test':testloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [resnet, alexnet, vgg, squeezenet, densenet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders,criterion, opt, epochs):\n",
    "    since = time.time()\n",
    "    avg_loss = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    print_freq = 100\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        running_loss = 0.0\n",
    "       \n",
    "\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "             # forward\n",
    "            # track history if only in train\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        # statistics\n",
    "            running_loss += loss.item() \n",
    "            \n",
    "            if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "                losses = running_loss / print_freq\n",
    "                \n",
    "                print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                    epoch, i, losses))\n",
    "                avg_loss.append(losses)\n",
    "                \n",
    "                running_loss = 0.0\n",
    "    \n",
    "                \n",
    "\n",
    "            \n",
    "            # deep copy the model\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        \n",
    "        \n",
    "        model_ft.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224 # what does it mean?\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.1.weight\n",
      "\t features.1.bias\n",
      "\t features.4.weight\n",
      "\t features.4.bias\n",
      "\t features.5.weight\n",
      "\t features.5.bias\n",
      "\t features.8.weight\n",
      "\t features.8.bias\n",
      "\t features.9.weight\n",
      "\t features.9.bias\n",
      "\t features.11.weight\n",
      "\t features.11.bias\n",
      "\t features.12.weight\n",
      "\t features.12.bias\n",
      "\t features.15.weight\n",
      "\t features.15.bias\n",
      "\t features.16.weight\n",
      "\t features.16.bias\n",
      "\t features.18.weight\n",
      "\t features.18.bias\n",
      "\t features.19.weight\n",
      "\t features.19.bias\n",
      "\t features.22.weight\n",
      "\t features.22.bias\n",
      "\t features.23.weight\n",
      "\t features.23.bias\n",
      "\t features.25.weight\n",
      "\t features.25.bias\n",
      "\t features.26.weight\n",
      "\t features.26.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n",
      "[epoch: 0, i:    99] avg mini-batch loss: 2.358\n",
      "[epoch: 0, i:   199] avg mini-batch loss: 2.228\n",
      "[epoch: 0, i:   299] avg mini-batch loss: 2.143\n",
      "[epoch: 0, i:   399] avg mini-batch loss: 2.169\n",
      "[epoch: 0, i:   499] avg mini-batch loss: 1.946\n",
      "[epoch: 0, i:   599] avg mini-batch loss: 2.013\n",
      "[epoch: 0, i:   699] avg mini-batch loss: 1.944\n",
      "[epoch: 0, i:   799] avg mini-batch loss: 1.909\n",
      "Epoch 1/39\n",
      "----------\n",
      "[epoch: 1, i:    99] avg mini-batch loss: 1.812\n",
      "[epoch: 1, i:   199] avg mini-batch loss: 1.680\n",
      "[epoch: 1, i:   299] avg mini-batch loss: 1.551\n",
      "[epoch: 1, i:   399] avg mini-batch loss: 1.693\n",
      "[epoch: 1, i:   499] avg mini-batch loss: 1.705\n",
      "[epoch: 1, i:   599] avg mini-batch loss: 1.575\n",
      "[epoch: 1, i:   699] avg mini-batch loss: 1.526\n",
      "[epoch: 1, i:   799] avg mini-batch loss: 1.415\n",
      "Epoch 2/39\n",
      "----------\n",
      "[epoch: 2, i:    99] avg mini-batch loss: 1.372\n",
      "[epoch: 2, i:   199] avg mini-batch loss: 1.249\n",
      "[epoch: 2, i:   299] avg mini-batch loss: 1.647\n",
      "[epoch: 2, i:   399] avg mini-batch loss: 1.355\n",
      "[epoch: 2, i:   499] avg mini-batch loss: 1.380\n",
      "[epoch: 2, i:   599] avg mini-batch loss: 1.324\n",
      "[epoch: 2, i:   699] avg mini-batch loss: 1.536\n",
      "[epoch: 2, i:   799] avg mini-batch loss: 1.560\n",
      "Epoch 3/39\n",
      "----------\n",
      "[epoch: 3, i:    99] avg mini-batch loss: 1.101\n",
      "[epoch: 3, i:   199] avg mini-batch loss: 0.891\n",
      "[epoch: 3, i:   299] avg mini-batch loss: 0.965\n",
      "[epoch: 3, i:   399] avg mini-batch loss: 0.870\n",
      "[epoch: 3, i:   499] avg mini-batch loss: 1.064\n",
      "[epoch: 3, i:   599] avg mini-batch loss: 1.166\n",
      "[epoch: 3, i:   699] avg mini-batch loss: 1.070\n",
      "[epoch: 3, i:   799] avg mini-batch loss: 1.258\n",
      "Epoch 4/39\n",
      "----------\n",
      "[epoch: 4, i:    99] avg mini-batch loss: 0.656\n",
      "[epoch: 4, i:   199] avg mini-batch loss: 0.886\n",
      "[epoch: 4, i:   299] avg mini-batch loss: 0.875\n",
      "[epoch: 4, i:   399] avg mini-batch loss: 0.934\n",
      "[epoch: 4, i:   499] avg mini-batch loss: 0.873\n",
      "[epoch: 4, i:   599] avg mini-batch loss: 0.882\n",
      "[epoch: 4, i:   699] avg mini-batch loss: 0.915\n",
      "[epoch: 4, i:   799] avg mini-batch loss: 0.851\n",
      "Epoch 5/39\n",
      "----------\n",
      "[epoch: 5, i:    99] avg mini-batch loss: 0.636\n",
      "[epoch: 5, i:   199] avg mini-batch loss: 0.512\n",
      "[epoch: 5, i:   299] avg mini-batch loss: 0.718\n",
      "[epoch: 5, i:   399] avg mini-batch loss: 0.504\n",
      "[epoch: 5, i:   499] avg mini-batch loss: 0.583\n",
      "[epoch: 5, i:   599] avg mini-batch loss: 0.497\n",
      "[epoch: 5, i:   699] avg mini-batch loss: 0.754\n",
      "[epoch: 5, i:   799] avg mini-batch loss: 0.838\n",
      "Epoch 6/39\n",
      "----------\n",
      "[epoch: 6, i:    99] avg mini-batch loss: 0.320\n",
      "[epoch: 6, i:   199] avg mini-batch loss: 0.228\n",
      "[epoch: 6, i:   299] avg mini-batch loss: 0.423\n",
      "[epoch: 6, i:   399] avg mini-batch loss: 0.658\n",
      "[epoch: 6, i:   499] avg mini-batch loss: 0.687\n",
      "[epoch: 6, i:   599] avg mini-batch loss: 0.606\n",
      "[epoch: 6, i:   699] avg mini-batch loss: 0.406\n",
      "[epoch: 6, i:   799] avg mini-batch loss: 0.548\n",
      "Epoch 7/39\n",
      "----------\n",
      "[epoch: 7, i:    99] avg mini-batch loss: 0.190\n",
      "[epoch: 7, i:   199] avg mini-batch loss: 0.080\n",
      "[epoch: 7, i:   299] avg mini-batch loss: 0.274\n",
      "[epoch: 7, i:   399] avg mini-batch loss: 0.547\n",
      "[epoch: 7, i:   499] avg mini-batch loss: 0.484\n",
      "[epoch: 7, i:   599] avg mini-batch loss: 0.478\n",
      "[epoch: 7, i:   699] avg mini-batch loss: 0.240\n",
      "[epoch: 7, i:   799] avg mini-batch loss: 0.492\n",
      "Epoch 8/39\n",
      "----------\n",
      "[epoch: 8, i:    99] avg mini-batch loss: 0.351\n",
      "[epoch: 8, i:   199] avg mini-batch loss: 0.099\n",
      "[epoch: 8, i:   299] avg mini-batch loss: 0.191\n",
      "[epoch: 8, i:   399] avg mini-batch loss: 0.285\n",
      "[epoch: 8, i:   499] avg mini-batch loss: 0.265\n",
      "[epoch: 8, i:   599] avg mini-batch loss: 0.718\n",
      "[epoch: 8, i:   699] avg mini-batch loss: 0.269\n",
      "[epoch: 8, i:   799] avg mini-batch loss: 0.369\n",
      "Epoch 9/39\n",
      "----------\n",
      "[epoch: 9, i:    99] avg mini-batch loss: 0.125\n",
      "[epoch: 9, i:   199] avg mini-batch loss: 0.077\n",
      "[epoch: 9, i:   299] avg mini-batch loss: 0.366\n",
      "[epoch: 9, i:   399] avg mini-batch loss: 0.149\n",
      "[epoch: 9, i:   499] avg mini-batch loss: 0.092\n",
      "[epoch: 9, i:   599] avg mini-batch loss: 0.172\n",
      "[epoch: 9, i:   699] avg mini-batch loss: 0.065\n",
      "[epoch: 9, i:   799] avg mini-batch loss: 0.261\n",
      "Epoch 10/39\n",
      "----------\n",
      "[epoch: 10, i:    99] avg mini-batch loss: 0.085\n",
      "[epoch: 10, i:   199] avg mini-batch loss: 0.052\n",
      "[epoch: 10, i:   299] avg mini-batch loss: 0.043\n",
      "[epoch: 10, i:   399] avg mini-batch loss: 0.117\n",
      "[epoch: 10, i:   499] avg mini-batch loss: 0.126\n",
      "[epoch: 10, i:   599] avg mini-batch loss: 0.232\n",
      "[epoch: 10, i:   699] avg mini-batch loss: 0.046\n",
      "[epoch: 10, i:   799] avg mini-batch loss: 0.303\n",
      "Epoch 11/39\n",
      "----------\n",
      "[epoch: 11, i:    99] avg mini-batch loss: 0.197\n",
      "[epoch: 11, i:   199] avg mini-batch loss: 0.083\n",
      "[epoch: 11, i:   299] avg mini-batch loss: 0.073\n",
      "[epoch: 11, i:   399] avg mini-batch loss: 0.042\n",
      "[epoch: 11, i:   499] avg mini-batch loss: 0.077\n",
      "[epoch: 11, i:   599] avg mini-batch loss: 0.078\n",
      "[epoch: 11, i:   699] avg mini-batch loss: 0.005\n",
      "[epoch: 11, i:   799] avg mini-batch loss: 0.015\n",
      "Epoch 12/39\n",
      "----------\n",
      "[epoch: 12, i:    99] avg mini-batch loss: 0.017\n",
      "[epoch: 12, i:   199] avg mini-batch loss: 0.047\n",
      "[epoch: 12, i:   299] avg mini-batch loss: 0.009\n",
      "[epoch: 12, i:   399] avg mini-batch loss: 0.004\n",
      "[epoch: 12, i:   499] avg mini-batch loss: 0.014\n",
      "[epoch: 12, i:   599] avg mini-batch loss: 0.003\n",
      "[epoch: 12, i:   699] avg mini-batch loss: 0.003\n",
      "[epoch: 12, i:   799] avg mini-batch loss: 0.001\n",
      "Epoch 13/39\n",
      "----------\n",
      "[epoch: 13, i:    99] avg mini-batch loss: 0.002\n",
      "[epoch: 13, i:   199] avg mini-batch loss: 0.013\n",
      "[epoch: 13, i:   299] avg mini-batch loss: 0.370\n",
      "[epoch: 13, i:   399] avg mini-batch loss: 0.037\n",
      "[epoch: 13, i:   499] avg mini-batch loss: 0.027\n",
      "[epoch: 13, i:   599] avg mini-batch loss: 0.049\n",
      "[epoch: 13, i:   699] avg mini-batch loss: 0.440\n",
      "[epoch: 13, i:   799] avg mini-batch loss: 0.027\n",
      "Epoch 14/39\n",
      "----------\n",
      "[epoch: 14, i:    99] avg mini-batch loss: 0.179\n",
      "[epoch: 14, i:   199] avg mini-batch loss: 0.221\n",
      "[epoch: 14, i:   299] avg mini-batch loss: 0.144\n",
      "[epoch: 14, i:   399] avg mini-batch loss: 0.229\n",
      "[epoch: 14, i:   499] avg mini-batch loss: 0.287\n",
      "[epoch: 14, i:   599] avg mini-batch loss: 0.183\n",
      "[epoch: 14, i:   699] avg mini-batch loss: 0.206\n",
      "[epoch: 14, i:   799] avg mini-batch loss: 0.057\n",
      "Epoch 15/39\n",
      "----------\n",
      "[epoch: 15, i:    99] avg mini-batch loss: 0.089\n",
      "[epoch: 15, i:   199] avg mini-batch loss: 0.050\n",
      "[epoch: 15, i:   299] avg mini-batch loss: 0.009\n",
      "[epoch: 15, i:   399] avg mini-batch loss: 0.006\n",
      "[epoch: 15, i:   499] avg mini-batch loss: 0.003\n",
      "[epoch: 15, i:   599] avg mini-batch loss: 0.010\n",
      "[epoch: 15, i:   699] avg mini-batch loss: 0.008\n",
      "[epoch: 15, i:   799] avg mini-batch loss: 0.008\n",
      "Epoch 16/39\n",
      "----------\n",
      "[epoch: 16, i:    99] avg mini-batch loss: 0.010\n",
      "[epoch: 16, i:   199] avg mini-batch loss: 0.002\n",
      "[epoch: 16, i:   299] avg mini-batch loss: 0.032\n",
      "[epoch: 16, i:   399] avg mini-batch loss: 0.010\n",
      "[epoch: 16, i:   499] avg mini-batch loss: 0.002\n",
      "[epoch: 16, i:   599] avg mini-batch loss: 0.002\n",
      "[epoch: 16, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 16, i:   799] avg mini-batch loss: 0.001\n",
      "Epoch 17/39\n",
      "----------\n",
      "[epoch: 17, i:    99] avg mini-batch loss: 0.001\n",
      "[epoch: 17, i:   199] avg mini-batch loss: 0.001\n",
      "[epoch: 17, i:   299] avg mini-batch loss: 0.014\n",
      "[epoch: 17, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 17, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 17, i:   599] avg mini-batch loss: 0.002\n",
      "[epoch: 17, i:   699] avg mini-batch loss: 0.014\n",
      "[epoch: 17, i:   799] avg mini-batch loss: 0.001\n",
      "Epoch 18/39\n",
      "----------\n",
      "[epoch: 18, i:    99] avg mini-batch loss: 0.001\n",
      "[epoch: 18, i:   199] avg mini-batch loss: 0.006\n",
      "[epoch: 18, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 18, i:   399] avg mini-batch loss: 0.001\n",
      "[epoch: 18, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 18, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 18, i:   699] avg mini-batch loss: 0.001\n",
      "[epoch: 18, i:   799] avg mini-batch loss: 0.031\n",
      "Epoch 19/39\n",
      "----------\n",
      "[epoch: 19, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 19, i:   199] avg mini-batch loss: 0.031\n",
      "[epoch: 19, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 19, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 19, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 19, i:   599] avg mini-batch loss: 0.001\n",
      "[epoch: 19, i:   699] avg mini-batch loss: 0.001\n",
      "[epoch: 19, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 20/39\n",
      "----------\n",
      "[epoch: 20, i:    99] avg mini-batch loss: 0.010\n",
      "[epoch: 20, i:   199] avg mini-batch loss: 0.001\n",
      "[epoch: 20, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 20, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 20, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 20, i:   599] avg mini-batch loss: 0.001\n",
      "[epoch: 20, i:   699] avg mini-batch loss: 0.035\n",
      "[epoch: 20, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 21/39\n",
      "----------\n",
      "[epoch: 21, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 21, i:   199] avg mini-batch loss: 0.001\n",
      "[epoch: 21, i:   299] avg mini-batch loss: 0.004\n",
      "[epoch: 21, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 21, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 21, i:   599] avg mini-batch loss: 0.018\n",
      "[epoch: 21, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 21, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 22/39\n",
      "----------\n",
      "[epoch: 22, i:    99] avg mini-batch loss: 0.016\n",
      "[epoch: 22, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 22, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 22, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 22, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 22, i:   599] avg mini-batch loss: 0.001\n",
      "[epoch: 22, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 22, i:   799] avg mini-batch loss: 0.014\n",
      "Epoch 23/39\n",
      "----------\n",
      "[epoch: 23, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 23, i:   199] avg mini-batch loss: 0.010\n",
      "[epoch: 23, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 23, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 23, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 23, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 23, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 23, i:   799] avg mini-batch loss: 0.021\n",
      "Epoch 24/39\n",
      "----------\n",
      "[epoch: 24, i:    99] avg mini-batch loss: 0.014\n",
      "[epoch: 24, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 24, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 24, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 24, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 24, i:   599] avg mini-batch loss: 0.008\n",
      "[epoch: 24, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 24, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 25/39\n",
      "----------\n",
      "[epoch: 25, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 25, i:   199] avg mini-batch loss: 0.001\n",
      "[epoch: 25, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 25, i:   399] avg mini-batch loss: 0.010\n",
      "[epoch: 25, i:   499] avg mini-batch loss: 0.015\n",
      "[epoch: 25, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 25, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 25, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 26/39\n",
      "----------\n",
      "[epoch: 26, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 26, i:   199] avg mini-batch loss: 0.004\n",
      "[epoch: 26, i:   299] avg mini-batch loss: 0.001\n",
      "[epoch: 26, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 26, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 26, i:   599] avg mini-batch loss: 0.026\n",
      "[epoch: 26, i:   699] avg mini-batch loss: 0.001\n",
      "[epoch: 26, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 27/39\n",
      "----------\n",
      "[epoch: 27, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 27, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 27, i:   299] avg mini-batch loss: 0.015\n",
      "[epoch: 27, i:   399] avg mini-batch loss: 0.011\n",
      "[epoch: 27, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 27, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 27, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 27, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 28/39\n",
      "----------\n",
      "[epoch: 28, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 28, i:   199] avg mini-batch loss: 0.003\n",
      "[epoch: 28, i:   299] avg mini-batch loss: 0.006\n",
      "[epoch: 28, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 28, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 28, i:   599] avg mini-batch loss: 0.015\n",
      "[epoch: 28, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 28, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 29/39\n",
      "----------\n",
      "[epoch: 29, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 29, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 29, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 29, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 29, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 29, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 29, i:   699] avg mini-batch loss: 0.006\n",
      "[epoch: 29, i:   799] avg mini-batch loss: 0.016\n",
      "Epoch 30/39\n",
      "----------\n",
      "[epoch: 30, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 30, i:   199] avg mini-batch loss: 0.005\n",
      "[epoch: 30, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 30, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 30, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 30, i:   599] avg mini-batch loss: 0.015\n",
      "[epoch: 30, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 30, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 31/39\n",
      "----------\n",
      "[epoch: 31, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 31, i:   199] avg mini-batch loss: 0.011\n",
      "[epoch: 31, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 31, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 31, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 31, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 31, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 31, i:   799] avg mini-batch loss: 0.008\n",
      "Epoch 32/39\n",
      "----------\n",
      "[epoch: 32, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 32, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 32, i:   299] avg mini-batch loss: 0.010\n",
      "[epoch: 32, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 32, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 32, i:   599] avg mini-batch loss: 0.014\n",
      "[epoch: 32, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 32, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 33/39\n",
      "----------\n",
      "[epoch: 33, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 33, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 33, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 33, i:   399] avg mini-batch loss: 0.026\n",
      "[epoch: 33, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 33, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 33, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 33, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 34/39\n",
      "----------\n",
      "[epoch: 34, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 34, i:   199] avg mini-batch loss: 0.013\n",
      "[epoch: 34, i:   299] avg mini-batch loss: 0.014\n",
      "[epoch: 34, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 34, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 34, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 34, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 34, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 35/39\n",
      "----------\n",
      "[epoch: 35, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 35, i:   199] avg mini-batch loss: 0.007\n",
      "[epoch: 35, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 35, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 35, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 35, i:   599] avg mini-batch loss: 0.009\n",
      "[epoch: 35, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 35, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 36/39\n",
      "----------\n",
      "[epoch: 36, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 36, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 36, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 36, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 36, i:   499] avg mini-batch loss: 0.011\n",
      "[epoch: 36, i:   599] avg mini-batch loss: 0.013\n",
      "[epoch: 36, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 36, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 37/39\n",
      "----------\n",
      "[epoch: 37, i:    99] avg mini-batch loss: 0.009\n",
      "[epoch: 37, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 37, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 37, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 37, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 37, i:   599] avg mini-batch loss: 0.018\n",
      "[epoch: 37, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 37, i:   799] avg mini-batch loss: 0.000\n",
      "Epoch 38/39\n",
      "----------\n",
      "[epoch: 38, i:    99] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   399] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 38, i:   799] avg mini-batch loss: 0.021\n",
      "Epoch 39/39\n",
      "----------\n",
      "[epoch: 39, i:    99] avg mini-batch loss: 0.003\n",
      "[epoch: 39, i:   199] avg mini-batch loss: 0.000\n",
      "[epoch: 39, i:   299] avg mini-batch loss: 0.000\n",
      "[epoch: 39, i:   399] avg mini-batch loss: 0.010\n",
      "[epoch: 39, i:   499] avg mini-batch loss: 0.000\n",
      "[epoch: 39, i:   599] avg mini-batch loss: 0.000\n",
      "[epoch: 39, i:   699] avg mini-batch loss: 0.000\n",
      "[epoch: 39, i:   799] avg mini-batch loss: 0.000\n",
      "Training complete in 12m 16s\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft,avg_loss = train_model(model_ft, trainloader, \n",
    "                             criterion, optimizer_ft, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZwcVbn/8c+TTBaSTPaQPQxIZAmBbIYt8IsaNaCCSlRcQFCJKFxBFARRQRARRBTMFYwiiCLbZQvXsF0Ii+wJJJAVEiAwJJKFhEwm+8zz++NUpXt6unt6kunp7unv+/WaV61d9UxNUk+fc6rOMXdHRETKV7tCByAiIoWlRCAiUuaUCEREypwSgYhImVMiEBEpcxWFDqC5+vbt61VVVYUOQ0SkpMyZM2eNu/dLt63kEkFVVRWzZ88udBgiIiXFzJZn2qaqIRGRMqdEICJS5pQIRETKXMm1EYiIAGzfvp3q6mq2bNlS6FCKSufOnRkyZAgdOnTI+TNKBCJSkqqrq6msrKSqqgozK3Q4RcHdWbt2LdXV1ey99945f05VQyJSkrZs2UKfPn2UBJKYGX369Gl2KUmJQERKlpJAY7tyTcomEcyfDxdeCGvXFjoSEZHiUjaJ4PXX4Ve/guUZX6kQEdl1F198MVdddVXG7atXr+bQQw9l9OjRPPXUU80+/k033cSZZ54JwL333svChQt3OdZUZZMI+vcP01WrChuHiJSnRx99lP3335+XX36Zo446areOpUSwi/bcM0yVCESkpVx22WXst99+TJo0iSVLlgCwbNkyJk+ezNixYznqqKNYvHgxc+fO5bzzzmPmzJmMGjWKzZs3893vfpdx48YxYsQILrroop3HrKqqYs2aNQDMnj2biRMnNjjnM888w4wZMzj33HMZNWoUy5Yt2+3fo2weH1UiEGm7zj4b5s5t2WOOGgW//33m7XPmzOG2227j5ZdfZseOHYwZM4axY8cydepUrr/+eoYPH87zzz/P9773PR577DEuueQSZs+ezbRp04CQRHr37k1dXR0f//jHeeWVVzj44IObjOuII47guOOO4zOf+QxTpkxpkd+1bBJBZSV07gzvvVfoSESkLXjqqaf4/Oc/T5cuXQA47rjj2LJlC8888wxf/OIXd+63devWtJ+/4447mD59Ojt27GDlypUsXLgwp0SQD2WTCMxCqUAlApG2J9s393xKfVSzvr6enj17MreJ4smbb77JVVddxYsvvkivXr045ZRTdj77X1FRQX19PUCrvTVdNm0EoEQgIi3n6KOP5p577mHz5s3U1NRw//3306VLF/bee2/uvPNOILzpO2/evEaf3bBhA127dqVHjx689957PPDAAzu3VVVVMWfOHADuuuuutOeurKykpqamxX6XskoE/furakhEWsaYMWP48pe/zKhRozjhhBN2Pgl0yy23cMMNN3DIIYcwYsQI7rvvvkafPeSQQxg9ejQjRozgm9/8JkceeeTObRdddBFnnXUWRx11FO3bt0977hNPPJHf/OY3jB49ukUai83dd/sgrWncuHG+qwPTfPOb8PDDUF3dwkGJSKtbtGgRBxxwQKHDKErpro2ZzXH3cen2L6sSQVw1VGK5T0Qkr8oqEfTvD9u3w/r1hY5ERKR4lFUiGDgwTFU1JNI2lFrVdmvYlWtSVolgn33C9M03CxuHiOy+zp07s3btWiWDJPF4BJ07d27W58rmPQJIJII33ihsHCKy+4YMGUJ1dTWrV68udChFJR6hrDnKKhH06QPdu0MLPG0lIgXWoUOHZo3CJZmVVdWQWSgVPPQQ/O53hY5GRKQ4lFUigJAIXn8dzjkHWvDFPBGRklV2iWDHjsS8EoGISBkmgpNOSswrEYiIlGEimDIF7r03zCsRiIiUYSKA8OQQKBGIiECZJoLKyjBVIhARKdNEoBKBiEhCWSaCuESwYUNh4xARKQZlnQhUIhARKdNE0LVreMtYiUBEpEwTgRl066ZEICICeUwEZjbUzGaZ2SIzW2BmZ6XZx8zsWjNbamavmNmYfMWTqrJSiUBEBPJbItgB/NDdDwAOA84wswNT9jkGGB79TAWuy2M8DVRWwgsvwK23ttYZRUSKU94SgbuvdPeXovkaYBEwOGW344GbPXgO6GlmA/MVU7LKSpg/H04+GbZta40ziogUp1ZpIzCzKmA08HzKpsHAO0nL1TROFpjZVDObbWazW2oQivjJoR07YOnSFjmkiEhJynsiMLNuwF3A2e6e+uS+pflIo3Hn3H26u49z93H9+vVrkbi6dk3ML1jQIocUESlJeU0EZtaBkARucfe70+xSDQxNWh4CrMhnTLG1axPzCxe2xhlFRIpTPp8aMuAGYJG7X51htxnAydHTQ4cBH7j7ynzFlOztt8O0XTslAhEpb/kcs/hI4CTgVTObG637CTAMwN2vB2YCxwJLgU3AqXmMp4E99gjTo44KjcYiIuUqb4nA3f9N+jaA5H0cOCNfMWQzcyY8/TSsWAEXXgjV1TBkSCEiEREprLJ8sxhg+HA45RQ44YSwfHe6FgwRkTJQtokgtt9+cNBBSgQiUr7KPhEATJigdgIRKV9KBMCwYeFx0traQkciItL6lAgIiQDgnXey7yci0hYpEZBIBPG7BSIi5USJACUCESlvSgTAoEHhDWMlAhEpR0oEQIcOMHiwEoGIlCclgsiwYbB8eaGjEBFpfUoEkUGDYGWrdHcnIlJclAgi/fvDe+8VOgoRkdanRBDZc09Yv17DVopI+VEiiPTvH6YtNBKmiEjJaFYiMLNeZnZwvoIppD33DFNVD4lIuWkyEZjZ42bW3cx6A/OAG80s04hjJStOBKtWFTYOEZHWlkuJoEc06PwXgBvdfSwwKb9htb64akiJQETKTS6JoMLMBgJfAv43z/EUjKqGRKRc5ZIILgEeApa6+4tmtg/wen7Dan3dukHnzqFEUFcHF1wQhrMUEWnrLAwbXDrGjRvns2fPzsuxq6rg6KOhd2+45ho4/HB45pm8nEpEpFWZ2Rx3H5duWy6NxVdGjcUdzOxRM1tjZl9v+TALb889Q4kgHrayU6fCxiMi0hpyqRr6ZNRY/BmgGvgwcG5eoyqQOBGsWROW1V4gIuUgl0TQIZoeC9zq7u/nMZ6C6t8f3nwTNm8Oy3qCSETKQS6J4H4zWwyMAx41s37AlvyGVRhxNxMAQ4eGcYx37Ehsr62FT34SXm9zTeUiUs6aTATufj5wODDO3bcDtcDx+Q6sEOJHSAFGjAjT5C4nli2DRx6B555r3bhERPIpl8biDsBJwO1m9j/At4C1+Q6sEOKXyiCRCJKrh7ZuDdNNm1ovJhGRfKvIYZ/rCO0Ef4yWT4rWfTtfQRVKuhJBcoOxEoGItEW5JIKPuPshScuPmdm8fAVUSOkSQXKJYEvUMqJEICJtSS6NxXVm9qF4IXqzuC5/IRVOXDVUUQEf/nCYV9WQiLR1uZQIzgVmmdkbgAF7AafmNaoC6dMHzKBvX+jRAzp2hP/8J7FdiUBE2qImE4G7P2pmw4H9CIlgsbtvzXtkBVBREZJBv34hIey/P8ydm9iermronXfgr3+Fn/88fEZEpNRkTARm9oUMmz5kZrj73XmKqaD69w8lAoAJE+Dmm8O7BBUV6UsEBx4IGzfCN74R+ioSESk12UoEn82yzYE2mQguvxy6dg3zEybAH/8Ir7wCY8Y0TgTLl4ckABrrWERKV8ZE4O671Q5gZn8l9E+0yt0PSrN9InAf8Ga06m53v2R3ztkSPpuU/o46KkyfeiokgtSqoaeeSuyrRCAipSqXxuJddRMwDbg5yz5Puftn8hjDbhkyBHr1gtdeC8upJYINGxL7KhGISKlq1uD1zeHuTwIl30Hd4MGwYkWYz5YItrbJ5nMRKQd5SwQ5OtzM5pnZA2Y2ItNOZjbVzGab2ezVyZ3/tIJBgxKJILVqSCUCEWkLcqoaMrMjgKrk/d09W5VPLl4C9nL3jWZ2LHAvMDzdju4+HZgOYYSy3TxvswwaBAsXhnlVDYlIW9RkIjCzvwMfAuaSeKPYyV7336RosJt4fqaZ/dHM+rr7mt05bksbNAhWroT6+saJoKYmsZ+qhkSkVOVSIhgHHOgtPLixmQ0A3nN3N7PxhGqqouvVdPDgMJj9qlWqGhKRtimXRDAfGACsbM6BzexWYCLQ18yqgYuIRjtz9+uBKcB3zWwHsBk4saWTTUsYNChMV6xoWCJwD4mgb98wtKUSgYiUqmxvFt9PqAKqBBaa2QvAzgoQdz8u24Hd/StNbJ9GeLy0qCUngrhEUF8fbvwbNoQuKdasUdWQiJSubCWCq1otiiKWrkQAoVRQUxNKBEuWqEQgIqUr25vFTwCY2d7ASnffEi3vAfTP9Lm2ZsAA6NQJFi1qnAg2bIADDgjLSgQiUqpyeY/gTqA+abkuWlcWKipCn0OPPpqoGoJEIog7qFPVkIiUqlwSQYW77/y+G813zF9IxWfSJHj1VXj77cS6X/wCamsTiUAlAhEpVbkkgtVmtrNh2MyOB4rqWf98mzQpTN96KwxWA3DLLWHap0+YKhGISKnK5fHR04FbzCx+wqeaMIB92Tj44MR89+7hKaFYr17Qvr2qhkSkdOWSCOrd/TAz6waYu9dEDchlo2NH6N0b3n8f9tuvYSKorAzbVSIQkVKVS9XQXQDuvtHd404V/id/IRWnAQPCdP/9Q6PxyJFhubIyPFWkRCAipSrbC2X7AyOAHinDVnYHOuc7sGIzYEDofK5z53DjP+SQ0IC8ZUsoEahqSERKVbaqof0II4z1pOGwlTXAafkMqhj1j96c6NQpTK+5Jgxyf+yxqhoSkdKW7YWy+4D7zOxwd3+2FWMqSnHVUJwIeveGq69OrFMiEJFSlUtj8ctmdgahmmhnlZC7fzNvURWhfv3CdMeOxttUNSQipSyXxuK/E3of/RTwBDCEUD1UVnr1CtN16xpvU9WQiJSyXBLBvu7+M6DW3f8GfBoYmd+wik/PnmG6fn3jbdmqhtasafi4qYhIscmlamh7NF1vZgcB/yEMW1lWsiWCbFVDcZVS8Y20ICIS5JIIpptZL+BnwAygWzRfVg47DLp1gx//uPE2tRGISClrMhG4+1+i2SeAffIbTvHq2bPhGMXJOnXKvE1EpNg12UZgZn3M7A9m9pKZzTGz35tZn9YIrlSoRCAipSyXxuLbgFXACYRxhtcAt+czqFKT6amhurrWj0VEpLlyaSPo7e6XJi3/0sw+l6+ASlGmp4ZUXSQipSCXEsEsMzvRzNpFP18C/pXvwEpJpqqhdE8YiYgUm2ydztUADhhwDuHFMiMkj43ARa0RYClIVzX03HPwyCOFiUdEpDmy9TVU2ZqBlLJ0VUO//CX8S+UmESkBuVQN7WRmF+cpjpKWrmronXcKE4uISHM1KxEAxzW9S/lJVzWkRCAipaK5icDyEkWJ69QJ6usTPZPW1qbvnE5EpBg1NxGMzUsUJa5jxzCNSwXV1Q23m9KniBSxbE8NnefuV5rZHwhPD8XrAXD37+c/vNIQD1azdSt06aJqIREpLdleKFsUTWe3RiClrFu3MN24MYxbkFoicA9vGbdv3/qxiYg0Jdvjo/dH07+1XjilqXv3MN2wIUzTlQi2b1ciEJHi1GQXE2b2YeBHhDEIdu7v7h/LX1ilJTURVFdD374wYQLce29Yl26ISxGRYpBLX0N3AtcDfwHUjVoaqYng/fdDIrjnHvj97+EHPwglAhGRYpRLItjh7tflPZISlpoIPvgAevQI8x06hKkSgYgUq1weH73fzL5nZgPNrHf809SHzOyvZrbKzOZn2G5mdq2ZLTWzV8xsTLOjLxJKBCJSynIpEXwjmp6btM5perSym4BpwM0Zth8DDI9+DgWui6YlJ10i2GuvMK9EICLFLpehKvfelQO7+5NmVpVll+OBm93dgefMrKeZDXT3lbtyvkKqjLrni8cfUIlAREpJthfKPubuj5nZF9Jtd/e7d/Pcg4HkBy2ro3WNEoGZTQWmAgwbNmw3T9vy2reHrl3TVw1VRFdYTw2JSLHKViL4f8BjwGfTbHNgdxNBuo4XPM063H06MB1g3LhxafcptO7dQyLYvh02b1aJQERKR7YXyi6Kpqfm6dzVwNCk5SHAijydK++6d4fHHoPzzw/LSgQiUipyeaGsJ3AyjV8o292+hmYAZ5rZbYRG4g9KsX0gVlkJs2fD1VeHZSUCESkVuTw1NBN4DngVqM/1wGZ2KzAR6Gtm1YShLTsAuPv10XGPBZYCm4B8lTxaRfzkUCxTIti2LbQbtGtuv68iInmSSyLo7O7nNPfA7v6VJrY7cEZzj1usckkE7nDAAXDqqfDTn7ZufCIimeTyvfTvZnZac18oKzepTwWle2po+XJ44w2YNSskhX/8A9asad04RURS5ZIItgG/AZ4F5kQ/6po6xcqU1o10JYKXXw7zc+fCs8/CSSfBOc0ua4mItKxcqobOAfZ1d313zWJFyvNOqYlg0yaYNy/Mv/8+XHVVmF+1qnXiExHJJJdEsIDQmCtZXHUVnH02rF4dllMTwZe+1HD/e+4J0+XLWyc+EZFMcqkaqgPmmtmfok7irjWza/MdWKn56lcbfruPE0A8jX3uc4n5yZPhtdfCC2giIoWSS4ng3uhHcvDEE/Dcc4nl5ERw3HFw441w1llQVQVz5sCDD8LChTB2bKuHKiIC5NbpnIaqbIajjw4/sYqkK3zwwdCzJ0ycGJa3bg1TJQIRKSS91pRnySWCeJD7WN++YbpuXfrP/vznYAb1Ob/GJyLSfEoEeZacCOLuqlOX415LU116aZiqewoRySclgjzLlgg6doROnRLjGGSybVvLxyUiEtulRBCNDyA5yFY1BCE5ZCoRxJQIRCSfdrVEkG4sAUkjW4kAQh9F6UoEcUMyqGpIRPJrlxKBu/+ppQNpq5KfGmpOieDNNxPzKhGISD7lMh5But5wPgDmuPvclg+pbbGkslNzSgTLliXmlQhEJJ9yKRGMA04njCc8mDB28ETgz2Z2Xv5Ca3vSJYJcSgSqGhKRfMolEfQBxrj7D939h4TE0A84Gjglj7G1OemqhjKVCD74IDGvEoGI5FMuiWAYoSvq2HZgL3ffDGxN/xFJJ1PVULoSwcaNiXmVCEQkn3JJBP8EnjOzi8zsIuBp4FYz6woszGt0bUxqB3QQkkNNTagK2ndfeOedsL62NrFPS5YI7rwTxo8PA+OIiEBufQ1damYzgQmEx0ZPd/d4YJqv5TO4ctC9exirYO7c0EC8cCEMHZq/RDB3Lrz4YihldOzYcscVkdKVy1ND1wC3u/s1rRBP2Ymri959N0zjaqJMVUOLF4flkSN37XxxUtm2TYlARIJcuqF+CfipmX0YuIeQFDRUZQuJB72vrg7TOBGkKxG4wwEHJOZ3Rfyi2tat6RuvRaT8NNlG4O5/c/djgfHAa8AVZvZ63iMrE6klgvgJotraxLY4ETz7bOJzu5oIkksEIiLQvDeL9wX2B6qAxXmJpgxlKxH07h3m46qhf/wj8bn33tu18yWXCEREILc2giuALwDLgDuAS919fb4Da0tuuikMSJNOtjaCXr3CmMbxt/e33kp8bulSGDCg+bGoRCAiqXJpI3gTONzd1+Q7mLbqG9/IvC21RJBcNdS/f5iPSwQbN8KQIWHfpUthwoTmxxInAJUIRCSWy+Oj15tZLzMbD3ROWv9kXiMrE/36hWk8gH1y1VCvXmE+vnnX1MBBB8HKlSER7Io4AahEICKxXKqGvg2cBQwB5gKHAc8CH8tvaOWhXz9o1y4xHGVq1RAkbtrxur322vVEoBKBiKTKpbH4LOAjwHJ3/ygwGlid16jKSPv2ibGLISSCurpwo44TQVw1VFMT2hSGDk20KTSXSgQikiqXRLDF3bcAmFknd18M7JffsMpLcqPvggXw1a+G+XQlgm7doEePpkc1y0QlAhFJlUtjcbWZ9QTuBR4xs3XAivyGVV7iRmGAdevgjjvCfHIiqK9PvFuwO4lAJQIRSZVLY/Hno9mLzWwW0AN4MK9RlZlMj4F27x7aD7ZvT7xp3K1bWJ/cTXVzqEQgIqmaNVSluz/h7jPcXd8nW1BcIrCUkaC7dQs9lm7bluh7KLlE8Nvfwn33hfXu8OMfw6uvZj+XSgQikmpXB6+XFhSXCJKriAC6dg0dw23fnni/IG4jqKuDH/0IPve5sH79erjySjj00Ozn0gtlIpIqr4nAzCab2RIzW2pm56fZPtHMPjCzudHPz/MZT7GKE0C6RJBaIoirhlJt2hSm8fsImaiLCRFJlUtj8S4xs/bAfwOfAKqBF81shrunDmbzlLt/Jl9xlIK4RJA6Elm3bqFEsG1bokRQWZm46SdLXufeuJopphKBiKTKZ4lgPLDU3d+I2hRuA47P4/lK1qGHhm4oTjih4fqOHRNVQ8klgh49Gh8judvqbB3SqUQgIqnymQgGA+8kLVdH61IdbmbzzOwBMxuR7kBmNtXMZpvZ7NWr2967bJWVoWO6iy6Chx+Gf/4zrB84MH1jcbqqoeRE8OKLYSQyM5g1q+F+KhGISKp8JoJ0lROpvei/BOzl7ocAfyC8q9D4Q+7T3X2cu4/rF3fO0wa1bw+f+AR85SuheqdHj8ZVQ5lKBMlVQ1OmwLRpYf7OOxPr3fX4qIg0ls9EUA0MTVoeQsqLaO6+wd03RvMzgQ5m1hfZKbVqKLVEEA83GZcIbrwx3Ozjx0iTG4+T2yBUIhCRWD4TwYvAcDPb28w6AicCM5J3MLMBZqFZM+rdtB2wNo8xlZy4aihTiSAebjJOBCOiyrX4G39yIki++atEICKxvCUCd98BnAk8BCwC7nD3BWZ2upmdHu02BZhvZvOAa4ET3Xd1EMa2Ka4a2rgROneGiorEYDYQqpMgUTUUP4IaN6UkJ4Lkm38xlgiqq+GHPwzvSIhI68nb46Ows7pnZsq665PmpwHT8hlDqevQIfFCWfztv337ML9xI2zZEtbFJYLKSujSJfHkUCmVCB54AK6+Gs44A/bZp9DRiJQPvVlc5JJLBHEigET1UGoi6NIl7Bd/qy6lEkH8uxRjkhJpy5QIilzcWPzBBw3bBuIxkLdvDzf9TZtCSaFjx4YJY926xHyxlwj0joNIYSgRFLm4sTg1EVxxBXz2s2F+y5ZQIujSJbw7kJwIkl+7UIlARNJRIihycdVQaiL49Kdh0qQwv2VLKBF07RqWkxPB2rWJYTBLpUQQJwQRaR1KBEUuU9UQwB57hGlcIogTQfJTRXV1oWdSaHjznzkTfv3rMD93bngbubWtXw8TJ8Jbb4VllQhECkOJoMhlqhqC8DgpNKwagoYlAkhUD6VWB11wQZiedx58//stG3cuFi+GJ56A2bPDstoIRApDiaDIZaoagoaJIFPVEMCaNWEa32CTt7uHR1PjUkNriksAqSUBJQKR1qVEUOQ6dAg36bq6pksEqYmgd+8wjbuniEsEyVVH27aFJBK/udya4gQQP+KqqiGRwlAiKHJxiQAa9zraVNVQ/JZx/I5BfJx4PwhJorY2DH3Z2lITgUoEIoWhRFDk4vcFIHOJYPPmhlVD8Tf+OBHEJYL4BpvcZXVtbfjsxo2Jp4ua4g6vvZb775BJaklAJQKRwlAiKHJVVYn5XJ8aaqpEkNwesHFjSATuDRNENo88AvvvD6+/nvOvkZZKBCLFQYmgyO21V2K+uU8NJZcIFiyAU09N7B+LEwHk3k6wYkVIHEuX5v57pJNrG8G118IPfrB75xKRzJQIily2EkFTTw3tuWeYbtwI06enP/66dYlxCnJtJ4hLDitWZN+vKbmWCM46C37/+907l4hkpkRQ5AYNSsxnSgTr14ebeZwA4mllZUgOtbUNSwFf/GJiPrkLiuQSwe9+B3PmpI8pTgTvvpv775FOattAuhJB8rsPeuNYJD+UCIpcRVJH4ZkSQdzldNywHDcWd+0afjZuhOXLYfhwePBBuPVWePnlsM+qVYnjxYnAHc49N4yjnE7c+NwaJYIFCxLzu5t4RCQ9JYISkvqiWGoiiBNFnAi6dUuMW7B8ORx8MHzqU6GX0rjaKDkRxFVDtbUNu6ZI9sYbLV8iyNZGkFwqqa7evfOJSHpKBCWkXcpfK1MiOOQQuPJKOPbYhiWC5PaGOKmkKxF88EGYJndhDaEU8aEPwaxZYXl3SwSZEsDWrSHm+fMTJRdQIhDJFyWCEjB/Ptx5Z+P1FRXh5z//CctxImjXLlTtVFaGG/6bb4abbvITSHHDcroSQZwIUksEixeH6aJFYdrSJYLkRDB5MowcCQsXwgEHhPXLl4dqKxFpWUoEJWDECJgyJf22zp0blwiSdeuWuIEnJ4L27cN7COlKBHFCSC0RxDf++Aa+alXiiaNdka1q6Omnw/yzz4YqrR494MIL4Qtf2PXziUh6SgQlrqlE0LVr4smb5KohCEkiW9VQaokgtWrGPVEa2RWZSgTLliX22bo1jF8cx3Tvvbt+PhFJT4mgxPXsmbihZioRxJJLBPG21Kqhd9+Ft98Oy5lKBMl2p50g+bHR+vpE6SLuljr2oQ+FN5kB+vTZ9fOJSHpKBCVu8ODEfGqndJBoC+jRo3Gi6NYt8W3cLJQIJk2CqVPDus2bGz7Bk1wiGDYsTHennSC5RJD6Eln79tCrV5jfZx94/HH46ldDI7LaCURalhJBiYtfOKusDDfPVHGJILVaKHkbhG/aq1fDkiUN90muHkq+6X/4w2G6OyWCOAlt3tz4ZbHevROlgH32Cd1ljBoVEkaufSKJSG6UCEpcXCJIVy0EiZt9arVQ8jaAoUPDcJWp37bjRFBX1/CmX1UVxkrIV4mgT5+QbCoqYMiQsK5v3zCNB9oRkZahRFDi4kRQV5d+e1w11FQiOPTQht1NxOJ2gvfea3iOykoYOLDl2ghSSwR9+sAPfwg33JAo6cTtA0oEIi1LiaDExVVDmapLslUNxV1SdOgA48en/3zcj9HXvx6W4x5Nu3YN526JEkE8SlqyPn3CewQnn5xYF5cI1q7d9XOKSGNKBCUuLhHE/f+kylYi+MhHwnT7dhg7Nv3n160L3TzMmgVXXBFe9IqPO3hwy5QIIPF4aMYUP+UAABDZSURBVNw9RnzTT6aqIZH8UCIocXEiyDS6WNynUNy4m+yooxLzBx4Y3klI7uQOQokgbkA+/vhEKaIlSwTxeSDx5FO6x0TjdQsWJBKHiOw+JYISN3Bg9u2f/CQ8/3yoZkl14IGJ+YoKOOEEOPHEhvvETxJVVISnd+JE0K1bSEIbNoQBampqYMaMzF1Xp7NlS6IEECeCOBGlSwTxuS+/HL797dzPIyLZKRGUuE6dQhXP9den396uXeb6/3btwmOasX/8A/7yl8TyIYeEbquXLAlJoEOHhiWC448Pz/pPnAhnnhmWjzkm9+f8N29OvCsQJ4IdO8I0XSJIfjz2gQc0pKVIS1EiaANeeAG+851d++zy5Q3fIO7UKUwnTAilg2efhYceSjzTHz+m2rVrWHfbbaF66Oabw/rVq3Orw9+xIzyFFCeCOIb47eKm3iCurYV//7vp84hI05QIyly3bolv+bFVq+DhhxPVRLW1YVAbaFg1BOFN5Pgt49NOC9O4k7tp0+Bvf0t/3rh9ID7eypVhGieC1JhiP/sZ/OQn0LEjzJzZ9O8nIk1TIpBG+vULPZNWVYVRyvbYI9zwAY48Mgx1OWpUWG7XLpRG+vWDs88O6xYvDt/wzz033LTTVRXFiSCumrrpppBQ4sbvuO0g1SWXwGWXwdFHh+qhTDZtgj/9qeFQl7tj+3a45ZbQSP3Pfzavm4szz4R//atl4hDJC3fP2w8wGVgCLAXOT7PdgGuj7a8AY5o65tixY11aV3199u11de61tWHaubP7f/2X+y9/6R5ul+6zZzc+xjvvhG3XXOM+fnyYv+EG9698JcwvX579nL/9bdjvrbeyb7/22tx+r6Z+xz//ORzv4IPD9MEHs+8fW7Ag7H/kkY231dU1vDYbNmSOM1N8O3a4X321+5w5ucUTH2/HjszbV650HzrU/d57G66vrc3t+Dt2NH093dP/vvX17lu25HaedNKd97LL3A87LP3vXFe36+dqTkz//rf71q3Z93vhhfB/IF+A2Z7pXp1pw+7+AO2BZcA+QEdgHnBgyj7HAg9ECeEw4PmmjqtEUNwGDEgkgJEj3du1c//yl92HD3c/5RT31avDf8j/+7+wz803u2/a5P7QQ+E/ZU2N+6xZTZ9n0aLw+dNPd3/mmTD9znfcL7wwnCeOoX9/97ffbvjZ7dvdp0xxHzcu3IxWrQo3+C9+McT24ovu773X8DMTJiSOCe5f+1rjmP75z3BTTr65/OIXYX8z9xUrwk3h9dfdt21zv/zyRLK68EL3jh3dn3664TG3bXOfPNl94sRw7Ftuabj9L38Jx2jXzv2mmxLrly51v+qqxjfb6dPdBw1yHzPGfePGxPqVK8O1d3f/6U/DMUePTtxYr7su/A6TJ4d9k23d6v7kk+4vv+y+fn24lscfnznZ1NaGv1e7du533OG+eHEi2X3tayG+d98N1/H8891/8pOG13TbNvcbb3R/7TX36upwrLlz3W+/Pfz7e+KJxL41Ne49eoTf5+67E+s3bnT/2MdCDN/5TuMEsnGj+623ht/H3X3+fPdf/SpxjWL19e6//rX7AQe4T50arsXvfuf+yCOJff7xj3D+738//fVwd9+82X3YsLDfo4+G3/fKK93/+79zS6q5yJYIzPPUlaOZHQ5c7O6fipYviEoglyft8yfgcXe/NVpeAkx095WZjjtu3DifndpPsRSNU04J7QI33BAeRz3zzPA0Ujx2MoT6/e3bQ1XQc8/BgAHNP497eELpoYfCcufO4RzJDdWnnQa33hqqiTp1CtVXdXVhed260ONqv35hefPmsK1nz/AEU+fO4T2J+vrw8/bbYXnFijBdtarhS3r19WEkOAi/T9zovmpVeJdj+fIwra8PMfbqlbgeZqEKq1076NKl4fXYvLnxuxr77hs+4x62jRwZfvcnnwwxmYU4N20KL+H16RP2rasLYz2MHx/6lRo4MFTB7dgR1nfqFNatXBniWLcuHK+iIjHm9ZIlYb++fRNpMT4XhOu5Zk1YX1WVuA7JVq+G998Pb6nHY2kMHRqu+euvh/j79g0xLF8etg8bFo4V95K7cmXYv1OnUF3XpUv4PbZtCw8yDB0aPrdpU/jb9ewZYorfjK+pCef+9Kfh/vtDV+fJ79CsWRPeYO/ZM/xOb78dnlLr16/hk3bbtoW/++jRYVjVvn3DZ83Cuztm4bPbt4frn/w+T/Ktd/PmsF/v3qFNbsCAxO++777hiT0Ij02fc07ja5oLM5vj7uPSbstjIpgCTHb3b0fLJwGHuvuZSfv8L/Brd/93tPwo8GN3n51yrKnAVIBhw4aNXR5fISk6NTWh/r9fv7DsDs88E/4jz5sXbjgrVoT/rKed1vR7EE1Ztiw82TRhQrjx1NWFG9jtt8O3vhW6zr7ppvAfbc2a8B+qY8dwM+zRA+6+O/xn/8pX4KWXwn/m8ePDEJnvvx8eWY1v0N//fkhwJ50EV12VeNQ1dtBB4T/w4483XP/tb8Ojj8Ibb4RzjxwZrkXHjqF95brrwg35S1+CP/6x8ahvEyeGG15dXbgBvvpqWG8W1l9wQbjeF10Ubl719eHFvEmT4L77EnGahdHuLrggXJ/77gvrIKzfsCEkrro6+MEPQgJfuzaRIC+9NFzva64J68wSN+2PfjS0Dc2bF0bTW748PM2WTseO4W8/ZAj89rfhJvz88yHuAw+Eww8PfzP38NKjWXhCLE48ZuFLwPPPh2R64olw113h73TKKTB9eqJnWwhPtx1xRDgmJI4zZUr4snLZZeElxWQdOsCnPhX+blu2hBv0xz4W/r2k9ut16KGhfWzatPBv/Zhjwr+f+ItB585w1llw440Nx/+I/yaxcePgs58N8VdXh3aw2tqG43Mcfzx87Wvpr2tTCpUIvgh8KiURjHf3/0ra51/A5SmJ4Dx3z/hakkoEIiLNly0R5POpoWpgaNLyECC1Z5pc9hERkTzKZyJ4ERhuZnubWUfgRGBGyj4zgJMtOAz4IFv7gIiItLyKpnfZNe6+w8zOBB4iPEH0V3dfYGanR9uvB2YSnhxaCmwCTs1XPCIikl7eEgGAu88k3OyT112fNO/AGfmMQUREstObxSIiZU6JQESkzCkRiIiUOSUCEZEyl7cXyvLFzFYDu/pqcV+glEe8VfyFVcrxl3LsoPhbwl7u3i/dhpJLBLvDzGZnerOuFCj+wirl+Es5dlD8+aaqIRGRMqdEICJS5sotEUwvdAC7SfEXVinHX8qxg+LPq7JqIxARkcbKrUQgIiIplAhERMpc2SQCM5tsZkvMbKmZnV/oeHJhZm+Z2atmNtfMZkfrepvZI2b2ejTtVeg4Aczsr2a2yszmJ63LGKuZXRD9LZaY2acKE3VChvgvNrN3o+s/18yOTdpWNPGb2VAzm2Vmi8xsgZmdFa0vieufJf5Suf6dzewFM5sXxf+LaH1JXH8gf4PXF9MPoRvsZcA+QEdgHnBgoePKIe63gL4p664Ezo/mzweuKHScUSxHA2OA+U3FChwY/Q06AXtHf5v2RRj/xcCP0uxbVPEDA4Ex0Xwl8FoUY0lc/yzxl8r1N6BbNN8BeB44rFSuv7uXTYlgPLDU3d9w923AbcDxBY5pVx0P/C2a/xvwuQLGspO7Pwm8n7I6U6zHA7e5+1Z3f5MwHsX4Vgk0gwzxZ1JU8bv7Snd/KZqvARYBgymR658l/kyKLX53943RYofoxymR6w/lUzU0GHgnabma7P/QioUDD5vZHDObGq3r79EobtF0z4JF17RMsZbS3+NMM3slqjqKi/ZFG7+ZVQGjCd9KS+76p8QPJXL9zay9mc0FVgGPuHtJXf9ySQSWZl0pPDd7pLuPAY4BzjCzowsdUAsplb/HdcCHgFHASuC30fqijN/MugF3AWe7+4Zsu6ZZV4zxl8z1d/c6dx9FGHd9vJkdlGX3oou/XBJBNTA0aXkIsKJAseTM3VdE01XAPYTi43tmNhAgmq4qXIRNyhRrSfw93P296D94PfBnEsX3oovfzDoQbqK3uPvd0eqSuf7p4i+l6x9z9/XA48BkSuj6l0sieBEYbmZ7m1lH4ERgRoFjysrMuppZZTwPfBKYT4j7G9Fu3wDuK0yEOckU6wzgRDPrZGZ7A8OBFwoQX1bxf+LI5wnXH4osfjMz4AZgkbtfnbSpJK5/pvhL6Pr3M7Oe0fwewCRgMSVy/YHyeGrIQ0v9sYSnEZYBFxY6nhzi3YfwZME8YEEcM9AHeBR4PZr2LnSsUVy3Eorv2wnfeL6VLVbgwuhvsQQ4pkjj/zvwKvAK4T/vwGKMH5hAqFp4BZgb/RxbKtc/S/ylcv0PBl6O4pwP/DxaXxLX393VxYSISLkrl6ohERHJQIlARKTMKRGIiJQ5JQIRkTKnRCAiUuaUCKTomNlxTfUQa2aDzOx/Mmx73MxyHijczEYl92yZZb+NOezTZOxpPnOTmU1pzmeyHOtwM/tzmvUPmtl6M/vflPV7m9nzUQ+Zt0fv2WDBtVEPma+Y2ZiWiE+KkxKBFB13n+Huv25inxXu3iI3T0IXBk0mglzkEnueTQYeTLP+N8BJadZfAfzO3YcD6wjvT0Do1mR49DOV0N2DtFFKBNJqzKzKzBab2V/MbL6Z3WJmk8zs6egb6fhov1PMbFo0f1P0zfQZM3sj/uYcHWt+ltN9PfrM/KTjjo/WvRxN94u+AV8CfNlCn/dfNrNuZnajhbEgXjGzE5J+h8ss9Dv/nJn1T/M75hK7mdk0M1toZv8iqeNAMxtrZk9Y6GjwITMbaGYVZvaimU2M9rnczC7L8Ht/HPi/1JXu/ihQkxKrAR8D4pJVag+ZN3vwHNAz5U1faUOUCKS17QtcQ3gbc3/gq4Q3S38E/CTDZwZG+3wGyPXbdld3PwL4HvDXaN1i4Gh3Hw38HPiVh27Jfw7c7u6j3P124GfAB+4+0t0PBh6Ljwk85+6HAE8Cp+UQR7rYPw/sB4yMjnEE7Oxv5w/AFHcfG8V9mbvvAE4BrjOzTxC+9f8i9URm1hfY7u4f5HKBCG++ro+ODw17wSy6HjIlfyoKHYCUnTfd/VUAM1sAPOrubmavAlUZPnOvh47HFqb7Fp7BrRDGGTCz7lFfMJXA38xsOKFLgw4ZPjuJ0B8V0THWRbPbgLiOfQ7wiRziSBf70cCt7l4HrDCzONHsBxwEPBK+rNOe0O0F7r7AzP4O3A8cHiWwVJ8EHs4hpli2XjCLrodMyR8lAmltW5Pm65OW68n87zH5M41uUGZ2I6EP+xXuHtf1p960HLgUmOXun7fQ7/3jGc5naT4P4dt2vL4uS7y5xJ7u+AYscPfDMxxrJLAeyJQMjwGuzrAtnTWEKp+KqFSQ3Atm0fWQKfmjqiEpee5+alStk9zg+2UAM5tAqOb5AOgBvBttPyVp3xpCaSH2MHBmvGAtPy70k4TeJ9tH9e4fjdYvAfqZ2eHReTuY2Yho/guEqpyjgWvj3i6TYjRCddvcXIOIktosIG50T+0h8+SoPeMwwjVc2fxfVUqBEoG0VevM7BngehJPwlwJXG5mTxOqXWKzgAPjxmLgl0CvqKF5HokbdUu5h9Aj5auEp3GeAIiqe6YAV0TnnQscEdX9/xr4lru/BkwjtLMkGwu8nFRiacDMngLuBD5uZtWWGDD9x8A5ZraUkGhuiNbPBN4gDKP4Z0Jbi7RR6n1UpA0ws58SxuW+rdCxSOlRIhARKXOqGhIRKXNKBCIiZU6JQESkzCkRiIiUOSUCEZEyp0QgIlLm/j8GHswcyRkuKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_loss, 'b', label='default')\n",
    "print_freq = 100\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net: Accuracy of the network on the 10000 test images: 11 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_ft(images)\n",
    "       \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print('Net: Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Accuracy of blues :  0 %\n",
      "Net Accuracy of classical : 13 %\n",
      "Net Accuracy of country :  8 %\n",
      "Net Accuracy of disco :  7 %\n",
      "Net Accuracy of hiphop :  0 %\n",
      "Net Accuracy of  jazz :  0 %\n",
      "Net Accuracy of metal :  0 %\n",
      "Net Accuracy of   pop : 20 %\n",
      "Net Accuracy of reggae : 26 %\n",
      "Net Accuracy of  rock : 40 %\n"
     ]
    }
   ],
   "source": [
    "classes = ('blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock')\n",
    "preds_net = []\n",
    "\n",
    "truths = []\n",
    "\n",
    "# Get test accuracy for each class.\n",
    "class_correct_net = list(0. for i in range(10))\n",
    "\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs_net = model_ft(images)\n",
    "        \n",
    "        _, predicted_net = torch.max(outputs_net, 1)\n",
    "        \n",
    "        c_net = (predicted_net == labels).squeeze()\n",
    "        \n",
    "        if type(c_net) is not list:\n",
    "            c_net = [c_net]\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            preds_net.append(predicted_net[i].item())\n",
    "            \n",
    "            truths.append(labels[i].item())\n",
    "#             confusion_matrix[actual][pred_net] += 1\n",
    "#             confusion_matrix_cnn[actual][pred_cnn] += 1\n",
    "            \n",
    "            label = labels[i]\n",
    "            class_correct_net[label] += c_net[i].item()\n",
    "            \n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Net Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct_net[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
