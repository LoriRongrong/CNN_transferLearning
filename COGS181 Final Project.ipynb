{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erika\\anaconda3\\envs\\Python3-7\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap_2d(training_errors, gamma_list, C_list, title, xlabel, ylabel, large=True):\n",
    "    if large:\n",
    "        plt.figure(figsize = (10,6))\n",
    "    ax = sns.heatmap(training_errors, annot=True, fmt='.3f', \n",
    "                     xticklabels=gamma_list, yticklabels=C_list)\n",
    "    ax.collections[0].colorbar.set_label(\"error\")\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(20*32*161, 100) # Get the vector of the layer right before the final fc layer and plot tfidf\n",
    "        self.fc2 = nn.Linear(100, 10) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20*32*161)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(X_train, Y_train, X_test, Y_test, draw=False):\n",
    "    net = NeuralNetRegressor(Net, max_epochs=100, lr=0.001, verbose=1)\n",
    "    \n",
    "    params = {\n",
    "        #'lr': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        #'max_epochs': list(range(500,5500, 500))\n",
    "        'lr': [0.001],\n",
    "        'max_epochs': [1, 3]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(net, params, refit=False, scoring='r2', verbose=1, cv=10)\n",
    "    \n",
    "    \n",
    "    #print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    \n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    \"\"\"\n",
    "    cross_val_errors = 1 - grid_search.cv_results_['mean_test_score'].reshape(1,2)\n",
    "    if draw:\n",
    "        draw_heatmap_2d(cross_val_errors, k_list, metric_list, title='cross-validation error w.r.t k and distance metric', ylabel='metric', xlabel='k', large=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best params: {}\".format(best_params))\n",
    "\n",
    "    best_test_accuracy = (Y_test == grid_search.best_estimator_.predict(X_test)).sum() / len(Y_test)\n",
    "    best_train_accuracy = (Y_train == grid_search.best_estimator_.predict(X_train)).sum() / len(Y_train)\n",
    "\n",
    "    return best_train_accuracy, best_test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][0]\n",
    "        y = self.data[idx][1]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        x = torch.Tensor(x)\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "raw_trainset = pickle.load(open('data/GTZAN.train', 'rb'))\n",
    "trainset = GTZANDataset(raw_trainset)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "raw_testset = pickle.load(open('data/GTZAN.test', 'rb'))\n",
    "testset = GTZANDataset(raw_testset)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "classes = ('blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnet = Net()     # Create the network instance.\\nnet.to(device)  # Move the network parameters to the specified device.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "# We use stochastic gradient descent (SGD) as optimizer.\n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['GTZAN']\n",
    "datasets = [trainset]\n",
    "models = [cnn]\n",
    "model_names = ['CNN']\n",
    "partition = 0.8\n",
    "num_trials = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: GTZAN\n",
      "% TRAINING: 0.8\n",
      "-------------------------------------\n",
      "TRIAL: 1\n",
      "---------\n",
      "MODEL: CNN\n",
      "(800, 1)\n",
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lr': 0.001, 'max_epochs': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-eb7ecd17a43f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MODEL: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TRIAL TRAIN ACCURACY FOR DATASET FOR MODEL: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2fc3af3518a0>\u001b[0m in \u001b[0;36mcnn\u001b[1;34m(X_train, Y_train, X_test, Y_test, draw)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best params: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mbest_test_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mbest_train_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(datasets):      \n",
    "    print('DATASET: ' + dataset_names[i])\n",
    "    print('% TRAINING: ' + str(partition))\n",
    "    print('-------------------------------------')\n",
    "\n",
    "    model_acc_sums = np.zeros((len(models), 2))\n",
    "    for k in range(num_trials):\n",
    "        print('TRIAL: %d' % (k+1))\n",
    "        print('---------')\n",
    "        \n",
    "        XY_train = list(zip(*raw_trainset))\n",
    "        X_train = np.array(XY_train[0])\n",
    "        Y_train = np.array(XY_train[1]).reshape(-1, 1)\n",
    "        \n",
    "        XY_test = list(zip(*raw_testset))\n",
    "        X_test = np.array(XY_train[0])\n",
    "        Y_test = np.array(XY_train[1]).reshape(-1, 1)\n",
    "        \n",
    "        for j, model in enumerate(models):\n",
    "            print('MODEL: %s' % model_names[j])\n",
    "\n",
    "            train_acc, test_acc = model(X_train, Y_train, X_test, Y_test, draw=True)\n",
    "\n",
    "            print('TRIAL TRAIN ACCURACY FOR DATASET FOR MODEL: ' + str(train_acc))\n",
    "            print('TRIAL TEST ACCURACY FOR DATASET FOR MODEL: ' + str(test_acc))\n",
    "            print()\n",
    "\n",
    "            model_acc_sums[j][0] += train_acc\n",
    "            model_acc_sums[j][1] += test_acc\n",
    "\n",
    "    print('-----RESULTS-----')\n",
    "    print('DATASET: ' + dataset_names[i])\n",
    "    print('% TRAINING: ' + str(partition))\n",
    "    print()\n",
    "    for j, model in enumerate(model_names):\n",
    "        print('MODEL: %s' % model_names[j])\n",
    "        avg_train_acc = round(model_acc_sums[j][0] / num_trials, 2)\n",
    "        avg_test_acc = round(model_acc_sums[j][1] / num_trials, 2)\n",
    "\n",
    "        print('AVG TRAIN ACCURACY FOR %s WITH %s DATASET WITH %s TRAINING SPLIT: %s' % (model_names[j], dataset_names[i], partition, str(avg_train_acc)))\n",
    "        print('AVG TEST ACCURACY FOR %s WITH %s DATASET WITH %s TRAINING SPLIT: %s' % (model_names[j], dataset_names[i], partition, str(avg_test_acc)))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
